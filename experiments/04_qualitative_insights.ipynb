{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Insights & Sentiment Analysis Experiment\n",
    "\n",
    "This notebook experiments with advanced sentiment analysis and qualitative insights extraction from TCS earnings call transcripts and financial reports.\n",
    "\n",
    "## Objectives:\n",
    "1. Analyze TCS earnings call transcripts for sentiment and key themes\n",
    "2. Extract management guidance and forward-looking statements\n",
    "3. Identify recurring themes and strategic initiatives\n",
    "4. Perform sentiment analysis on quarterly communications\n",
    "5. Generate structured qualitative insights for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Text processing and NLP\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Advanced NLP models\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Topic modeling and clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import umap\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# API integration\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")\n",
    "print(f\"🤗 Transformers available: {torch.__version__}\")\n",
    "print(f\"🔬 Advanced NLP pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "PDFS_DIR = os.path.join(DATA_DIR, \"pdfs\")\n",
    "OUTPUT_DIR = \"outputs/qualitative_analysis\"\n",
    "\n",
    "# Model configuration\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY', 'your-api-key-here')\n",
    "CLAUDE_MODEL = \"claude-3-5-sonnet-20241022\"\n",
    "\n",
    "# NLP model configurations\n",
    "SENTIMENT_MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "FINANCIAL_SENTIMENT_MODEL = \"ProsusAI/finbert\"\n",
    "\n",
    "# Analysis parameters\n",
    "MIN_SENTENCE_LENGTH = 10\n",
    "MAX_TOPICS = 10\n",
    "SENTIMENT_THRESHOLD = 0.1\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"💾 Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"🤖 Claude model: {CLAUDE_MODEL}\")\n",
    "print(f\"💭 Sentiment model: {SENTIMENT_MODEL}\")\n",
    "print(f\"🎯 Financial model: {FINANCIAL_SENTIMENT_MODEL}\")\n",
    "print(f\"🔑 API configured: {'✅' if ANTHROPIC_API_KEY != 'your-api-key-here' else '❌ Need API key'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP models and tools\n",
    "def initialize_nlp_models():\n",
    "    \"\"\"\n",
    "    Initialize all NLP models and tools\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    try:\n",
    "        # Download required NLTK data\n",
    "        nltk.download('vader_lexicon', quiet=True)\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        nltk.download('wordnet', quiet=True)\n",
    "        \n",
    "        # Initialize NLTK tools\n",
    "        models['nltk_sentiment'] = SentimentIntensityAnalyzer()\n",
    "        models['lemmatizer'] = WordNetLemmatizer()\n",
    "        models['stop_words'] = set(stopwords.words('english'))\n",
    "        \n",
    "        print(\"✅ NLTK models initialized\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"NLTK initialization failed: {e}\")\n",
    "        print(\"❌ NLTK models failed to load\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize transformer models\n",
    "        models['sentiment_pipeline'] = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=SENTIMENT_MODEL, \n",
    "            tokenizer=SENTIMENT_MODEL\n",
    "        )\n",
    "        print(\"✅ RoBERTa sentiment model loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sentiment model loading failed: {e}\")\n",
    "        print(\"❌ Sentiment model failed to load\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize financial sentiment model\n",
    "        models['financial_sentiment'] = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=FINANCIAL_SENTIMENT_MODEL,\n",
    "            tokenizer=FINANCIAL_SENTIMENT_MODEL\n",
    "        )\n",
    "        print(\"✅ FinBERT financial sentiment model loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Financial sentiment model loading failed: {e}\")\n",
    "        print(\"❌ Financial sentiment model failed to load\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize sentence embeddings\n",
    "        models['sentence_transformer'] = SentenceTransformer(EMBEDDING_MODEL)\n",
    "        print(\"✅ Sentence transformer loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sentence transformer loading failed: {e}\")\n",
    "        print(\"❌ Sentence transformer failed to load\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize spaCy for NER\n",
    "        models['nlp'] = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"✅ spaCy model loaded\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"spaCy loading failed: {e}\")\n",
    "        print(\"❌ spaCy model failed to load (install: python -m spacy download en_core_web_sm)\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Claude client\n",
    "        if ANTHROPIC_API_KEY != 'your-api-key-here':\n",
    "            models['claude_client'] = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "            print(\"✅ Claude API client initialized\")\n",
    "        else:\n",
    "            print(\"⚠️ Claude API key not configured\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Claude client initialization failed: {e}\")\n",
    "        print(\"❌ Claude client failed to initialize\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize all models\n",
    "print(\"🚀 Initializing NLP models...\")\n",
    "nlp_models = initialize_nlp_models()\n",
    "print(f\"\\n📊 Models loaded: {len(nlp_models)}/7 expected models\")\n",
    "\n",
    "# Show available models\n",
    "print(\"\\n🔧 Available analysis tools:\")\n",
    "for model_name in nlp_models.keys():\n",
    "    print(f\"  • {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess text data\n",
    "def extract_text_from_pdfs() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract text content from TCS financial PDFs\n",
    "    \"\"\"\n",
    "    import fitz  # PyMuPDF\n",
    "    \n",
    "    text_data = {}\n",
    "    pdf_files = [f for f in os.listdir(PDFS_DIR) if f.endswith('.pdf')]\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            pdf_path = os.path.join(PDFS_DIR, pdf_file)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            full_text = \"\"\n",
    "            for page_num in range(min(len(doc), 10)):  # Limit to first 10 pages\n",
    "                page = doc.load_page(page_num)\n",
    "                full_text += page.get_text()\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "            if len(full_text.strip()) > 100:  # Only keep substantial text\n",
    "                text_data[pdf_file] = full_text\n",
    "                print(f\"📄 Extracted {len(full_text)} characters from {pdf_file}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Minimal text found in {pdf_file}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {pdf_file}: {e}\")\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "def preprocess_text(text: str, nlp_models: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Preprocess text for analysis\n",
    "    \"\"\"\n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)  # Remove special characters\n",
    "    \n",
    "    # Sentence tokenization\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [s for s in sentences if len(s) >= MIN_SENTENCE_LENGTH]\n",
    "    \n",
    "    # Word tokenization and cleaning\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    if 'stop_words' in nlp_models and 'lemmatizer' in nlp_models:\n",
    "        words = [\n",
    "            nlp_models['lemmatizer'].lemmatize(word) \n",
    "            for word in words \n",
    "            if word.isalpha() and word not in nlp_models['stop_words']\n",
    "        ]\n",
    "    \n",
    "    # Extract financial keywords\n",
    "    financial_keywords = [\n",
    "        'revenue', 'profit', 'margin', 'growth', 'earnings', 'guidance', \n",
    "        'outlook', 'performance', 'strategy', 'investment', 'market', \n",
    "        'client', 'digital', 'transformation', 'ai', 'cloud', 'services'\n",
    "    ]\n",
    "    \n",
    "    found_keywords = [word for word in words if word in financial_keywords]\n",
    "    \n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'sentences': sentences,\n",
    "        'words': words,\n",
    "        'financial_keywords': found_keywords,\n",
    "        'sentence_count': len(sentences),\n",
    "        'word_count': len(words),\n",
    "        'keyword_count': len(found_keywords)\n",
    "    }\n",
    "\n",
    "# Extract and preprocess text data\n",
    "print(\"📖 Extracting text from TCS documents...\")\n",
    "document_texts = extract_text_from_pdfs()\n",
    "\n",
    "print(f\"\\n📚 Found {len(document_texts)} documents with substantial text\")\n",
    "\n",
    "# Preprocess all documents\n",
    "preprocessed_docs = {}\n",
    "for doc_name, text in document_texts.items():\n",
    "    print(f\"🔄 Preprocessing {doc_name}...\")\n",
    "    preprocessed = preprocess_text(text, nlp_models)\n",
    "    preprocessed_docs[doc_name] = preprocessed\n",
    "    \n",
    "    print(f\"  📊 {preprocessed['sentence_count']} sentences, {preprocessed['word_count']} words, {preprocessed['keyword_count']} financial keywords\")\n",
    "\n",
    "print(f\"\\n✅ Preprocessed {len(preprocessed_docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced sentiment analysis\n",
    "def analyze_sentiment_comprehensive(text: str, nlp_models: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive sentiment analysis using multiple models\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'text_length': len(text),\n",
    "        'sentiment_scores': {},\n",
    "        'confidence_scores': {},\n",
    "        'dominant_sentiment': None\n",
    "    }\n",
    "    \n",
    "    # NLTK VADER sentiment\n",
    "    if 'nltk_sentiment' in nlp_models:\n",
    "        try:\n",
    "            vader_scores = nlp_models['nltk_sentiment'].polarity_scores(text)\n",
    "            results['sentiment_scores']['vader'] = {\n",
    "                'positive': vader_scores['pos'],\n",
    "                'negative': vader_scores['neg'],\n",
    "                'neutral': vader_scores['neu'],\n",
    "                'compound': vader_scores['compound']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"VADER sentiment analysis failed: {e}\")\n",
    "    \n",
    "    # RoBERTa sentiment\n",
    "    if 'sentiment_pipeline' in nlp_models:\n",
    "        try:\n",
    "            # Truncate text if too long\n",
    "            truncated_text = text[:512] if len(text) > 512 else text\n",
    "            roberta_result = nlp_models['sentiment_pipeline'](truncated_text)[0]\n",
    "            \n",
    "            results['sentiment_scores']['roberta'] = {\n",
    "                'label': roberta_result['label'].lower(),\n",
    "                'score': roberta_result['score']\n",
    "            }\n",
    "            results['confidence_scores']['roberta'] = roberta_result['score']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"RoBERTa sentiment analysis failed: {e}\")\n",
    "    \n",
    "    # Financial sentiment (FinBERT)\n",
    "    if 'financial_sentiment' in nlp_models:\n",
    "        try:\n",
    "            # Truncate text if too long\n",
    "            truncated_text = text[:512] if len(text) > 512 else text\n",
    "            finbert_result = nlp_models['financial_sentiment'](truncated_text)[0]\n",
    "            \n",
    "            results['sentiment_scores']['finbert'] = {\n",
    "                'label': finbert_result['label'].lower(),\n",
    "                'score': finbert_result['score']\n",
    "            }\n",
    "            results['confidence_scores']['finbert'] = finbert_result['score']\n",
    "        except Exception as e:\n",
    "            logger.error(f\"FinBERT sentiment analysis failed: {e}\")\n",
    "    \n",
    "    # Determine dominant sentiment\n",
    "    sentiment_votes = []\n",
    "    \n",
    "    if 'vader' in results['sentiment_scores']:\n",
    "        compound = results['sentiment_scores']['vader']['compound']\n",
    "        if compound >= SENTIMENT_THRESHOLD:\n",
    "            sentiment_votes.append('positive')\n",
    "        elif compound <= -SENTIMENT_THRESHOLD:\n",
    "            sentiment_votes.append('negative')\n",
    "        else:\n",
    "            sentiment_votes.append('neutral')\n",
    "    \n",
    "    for model in ['roberta', 'finbert']:\n",
    "        if model in results['sentiment_scores']:\n",
    "            sentiment_votes.append(results['sentiment_scores'][model]['label'])\n",
    "    \n",
    "    if sentiment_votes:\n",
    "        # Most common sentiment\n",
    "        from collections import Counter\n",
    "        sentiment_counts = Counter(sentiment_votes)\n",
    "        results['dominant_sentiment'] = sentiment_counts.most_common(1)[0][0]\n",
    "        results['sentiment_consensus'] = sentiment_counts.most_common(1)[0][1] / len(sentiment_votes)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_document_sentiment(doc_data: Dict, nlp_models: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze sentiment for entire document and by sentences\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'document_sentiment': None,\n",
    "        'sentence_sentiments': [],\n",
    "        'sentiment_distribution': {},\n",
    "        'financial_sentiment_highlights': []\n",
    "    }\n",
    "    \n",
    "    # Overall document sentiment\n",
    "    full_text = doc_data['original_text'][:2000]  # Limit for API calls\n",
    "    results['document_sentiment'] = analyze_sentiment_comprehensive(full_text, nlp_models)\n",
    "    \n",
    "    # Sentence-level sentiment analysis\n",
    "    for i, sentence in enumerate(doc_data['sentences'][:50]):  # Limit to first 50 sentences\n",
    "        if len(sentence) >= MIN_SENTENCE_LENGTH:\n",
    "            sent_analysis = analyze_sentiment_comprehensive(sentence, nlp_models)\n",
    "            sent_analysis['sentence_index'] = i\n",
    "            sent_analysis['sentence_text'] = sentence[:200]  # Truncate for storage\n",
    "            results['sentence_sentiments'].append(sent_analysis)\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    if results['sentence_sentiments']:\n",
    "        sentiments = [s['dominant_sentiment'] for s in results['sentence_sentiments'] if s['dominant_sentiment']]\n",
    "        \n",
    "        from collections import Counter\n",
    "        sentiment_counts = Counter(sentiments)\n",
    "        total_sentences = len(sentiments)\n",
    "        \n",
    "        if total_sentences > 0:\n",
    "            results['sentiment_distribution'] = {\n",
    "                sentiment: count / total_sentences \n",
    "                for sentiment, count in sentiment_counts.items()\n",
    "            }\n",
    "    \n",
    "    # Extract highly positive/negative financial sentences\n",
    "    for sent_data in results['sentence_sentiments']:\n",
    "        if ('finbert' in sent_data['sentiment_scores'] and \n",
    "            sent_data['confidence_scores'].get('finbert', 0) > 0.8):\n",
    "            \n",
    "            results['financial_sentiment_highlights'].append({\n",
    "                'sentence': sent_data['sentence_text'],\n",
    "                'sentiment': sent_data['sentiment_scores']['finbert']['label'],\n",
    "                'confidence': sent_data['confidence_scores']['finbert'],\n",
    "                'index': sent_data['sentence_index']\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze sentiment for all documents\n",
    "print(\"💭 Starting comprehensive sentiment analysis...\")\n",
    "sentiment_results = {}\n",
    "\n",
    "for doc_name, doc_data in preprocessed_docs.items():\n",
    "    print(f\"\\n🔍 Analyzing sentiment for {doc_name}...\")\n",
    "    \n",
    "    sentiment_analysis = analyze_document_sentiment(doc_data, nlp_models)\n",
    "    sentiment_results[doc_name] = sentiment_analysis\n",
    "    \n",
    "    # Display summary\n",
    "    doc_sentiment = sentiment_analysis['document_sentiment']\n",
    "    if doc_sentiment and 'dominant_sentiment' in doc_sentiment:\n",
    "        consensus = sentiment_analysis['document_sentiment'].get('sentiment_consensus', 0)\n",
    "        print(f\"  📊 Overall sentiment: {doc_sentiment['dominant_sentiment']} (consensus: {consensus:.2f})\")\n",
    "    \n",
    "    if sentiment_analysis['sentiment_distribution']:\n",
    "        print(f\"  📈 Distribution: {sentiment_analysis['sentiment_distribution']}\")\n",
    "    \n",
    "    highlights_count = len(sentiment_analysis['financial_sentiment_highlights'])\n",
    "    print(f\"  💡 High-confidence financial sentiments: {highlights_count}\")\n",
    "\n",
    "print(f\"\\n✅ Sentiment analysis completed for {len(sentiment_results)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling and theme extraction\n",
    "def extract_topics_and_themes(preprocessed_docs: Dict, nlp_models: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract topics and themes using LDA and clustering\n",
    "    \"\"\"\n",
    "    # Combine all documents for topic modeling\n",
    "    all_sentences = []\n",
    "    sentence_sources = []\n",
    "    \n",
    "    for doc_name, doc_data in preprocessed_docs.items():\n",
    "        for sentence in doc_data['sentences'][:30]:  # Limit sentences per doc\n",
    "            if len(sentence) >= MIN_SENTENCE_LENGTH:\n",
    "                all_sentences.append(sentence)\n",
    "                sentence_sources.append(doc_name)\n",
    "    \n",
    "    if len(all_sentences) < 10:\n",
    "        print(\"⚠️ Insufficient data for topic modeling\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"📝 Analyzing {len(all_sentences)} sentences for topic modeling\")\n",
    "    \n",
    "    results = {\n",
    "        'sentence_count': len(all_sentences),\n",
    "        'lda_topics': {},\n",
    "        'sentence_clusters': {},\n",
    "        'key_themes': [],\n",
    "        'financial_themes': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # TF-IDF Vectorization\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=1000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.8\n",
    "        )\n",
    "        \n",
    "        tfidf_matrix = vectorizer.fit_transform(all_sentences)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        print(f\"📊 TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "        \n",
    "        # LDA Topic Modeling\n",
    "        n_topics = min(MAX_TOPICS, len(all_sentences) // 5)\n",
    "        if n_topics >= 2:\n",
    "            lda = LatentDirichletAllocation(\n",
    "                n_components=n_topics,\n",
    "                random_state=42,\n",
    "                max_iter=100\n",
    "            )\n",
    "            \n",
    "            lda.fit(tfidf_matrix)\n",
    "            \n",
    "            # Extract topics\n",
    "            topics = []\n",
    "            for topic_idx, topic in enumerate(lda.components_):\n",
    "                top_words_idx = topic.argsort()[-10:][::-1]\n",
    "                top_words = [feature_names[i] for i in top_words_idx]\n",
    "                \n",
    "                topics.append({\n",
    "                    'topic_id': topic_idx,\n",
    "                    'keywords': top_words,\n",
    "                    'weight': float(topic.max())\n",
    "                })\n",
    "            \n",
    "            results['lda_topics'] = {\n",
    "                'n_topics': n_topics,\n",
    "                'topics': topics,\n",
    "                'perplexity': lda.perplexity(tfidf_matrix)\n",
    "            }\n",
    "            \n",
    "            print(f\"🎯 Extracted {n_topics} topics with perplexity: {lda.perplexity(tfidf_matrix):.2f}\")\n",
    "        \n",
    "        # Sentence clustering using embeddings\n",
    "        if 'sentence_transformer' in nlp_models:\n",
    "            print(\"🔄 Generating sentence embeddings...\")\n",
    "            \n",
    "            # Limit sentences for embedding (computational constraint)\n",
    "            sample_sentences = all_sentences[:100]\n",
    "            embeddings = nlp_models['sentence_transformer'].encode(sample_sentences)\n",
    "            \n",
    "            # K-means clustering\n",
    "            n_clusters = min(8, len(sample_sentences) // 10)\n",
    "            if n_clusters >= 2:\n",
    "                kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                cluster_labels = kmeans.fit_predict(embeddings)\n",
    "                \n",
    "                # Group sentences by cluster\n",
    "                clusters = {}\n",
    "                for i, (sentence, label) in enumerate(zip(sample_sentences, cluster_labels)):\n",
    "                    if label not in clusters:\n",
    "                        clusters[label] = []\n",
    "                    clusters[label].append({\n",
    "                        'sentence': sentence[:200],\n",
    "                        'source': sentence_sources[i]\n",
    "                    })\n",
    "                \n",
    "                results['sentence_clusters'] = {\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'clusters': clusters,\n",
    "                    'sample_size': len(sample_sentences)\n",
    "                }\n",
    "                \n",
    "                print(f\"🎪 Created {n_clusters} sentence clusters\")\n",
    "        \n",
    "        # Extract financial themes\n",
    "        financial_keywords = {\n",
    "            'growth': ['growth', 'expansion', 'increase', 'rising', 'uptick'],\n",
    "            'performance': ['performance', 'results', 'achievement', 'success'],\n",
    "            'market': ['market', 'industry', 'sector', 'competition'],\n",
    "            'strategy': ['strategy', 'initiative', 'plan', 'approach'],\n",
    "            'technology': ['digital', 'ai', 'cloud', 'technology', 'innovation'],\n",
    "            'challenges': ['challenge', 'risk', 'concern', 'difficulty'],\n",
    "            'opportunities': ['opportunity', 'potential', 'prospect', 'future']\n",
    "        }\n",
    "        \n",
    "        theme_scores = {}\n",
    "        for theme, keywords in financial_keywords.items():\n",
    "            score = 0\n",
    "            for sentence in all_sentences:\n",
    "                sentence_lower = sentence.lower()\n",
    "                for keyword in keywords:\n",
    "                    score += sentence_lower.count(keyword)\n",
    "            \n",
    "            if score > 0:\n",
    "                theme_scores[theme] = score\n",
    "        \n",
    "        results['financial_themes'] = sorted(\n",
    "            theme_scores.items(), \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        print(f\"💰 Identified {len(results['financial_themes'])} financial themes\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Topic modeling failed: {e}\")\n",
    "        print(f\"❌ Topic modeling failed: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Extract topics and themes\n",
    "print(\"🎯 Starting topic modeling and theme extraction...\")\n",
    "topic_results = extract_topics_and_themes(preprocessed_docs, nlp_models)\n",
    "\n",
    "if topic_results:\n",
    "    print(\"\\n📋 Topic Modeling Results:\")\n",
    "    \n",
    "    if 'lda_topics' in topic_results and topic_results['lda_topics']:\n",
    "        topics = topic_results['lda_topics']['topics']\n",
    "        print(f\"  🎯 {len(topics)} LDA topics identified\")\n",
    "        \n",
    "        for i, topic in enumerate(topics[:3]):  # Show first 3 topics\n",
    "            keywords = ', '.join(topic['keywords'][:5])\n",
    "            print(f\"    Topic {i+1}: {keywords}\")\n",
    "    \n",
    "    if 'financial_themes' in topic_results and topic_results['financial_themes']:\n",
    "        print(f\"\\n💰 Top Financial Themes:\")\n",
    "        for theme, score in topic_results['financial_themes'][:5]:\n",
    "            print(f\"    {theme.title()}: {score} mentions\")\n",
    "    \n",
    "    if 'sentence_clusters' in topic_results and topic_results['sentence_clusters']:\n",
    "        n_clusters = topic_results['sentence_clusters']['n_clusters']\n",
    "        print(f\"\\n🎪 {n_clusters} sentence clusters created\")\n",
    "\n",
    "print(\"\\n✅ Topic modeling completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced qualitative insights with Claude\n",
    "def generate_qualitative_insights_with_claude(document_data: Dict, sentiment_data: Dict, topic_data: Dict, claude_client) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate advanced qualitative insights using Claude\n",
    "    \"\"\"\n",
    "    if claude_client is None:\n",
    "        return generate_fallback_insights(document_data, sentiment_data, topic_data)\n",
    "    \n",
    "    try:\n",
    "        # Prepare context for Claude\n",
    "        context = {\n",
    "            'document_count': len(document_data),\n",
    "            'total_sentences': sum(doc['sentence_count'] for doc in document_data.values()),\n",
    "            'sentiment_summary': {},\n",
    "            'topic_summary': {},\n",
    "            'sample_content': {}\n",
    "        }\n",
    "        \n",
    "        # Summarize sentiment data\n",
    "        all_sentiments = []\n",
    "        for doc_name, sent_data in sentiment_data.items():\n",
    "            if sent_data.get('document_sentiment', {}).get('dominant_sentiment'):\n",
    "                all_sentiments.append(sent_data['document_sentiment']['dominant_sentiment'])\n",
    "        \n",
    "        if all_sentiments:\n",
    "            from collections import Counter\n",
    "            sentiment_counts = Counter(all_sentiments)\n",
    "            context['sentiment_summary'] = dict(sentiment_counts)\n",
    "        \n",
    "        # Summarize topic data\n",
    "        if 'lda_topics' in topic_data and topic_data['lda_topics']:\n",
    "            topics = topic_data['lda_topics']['topics']\n",
    "            context['topic_summary']['lda_topics'] = [\n",
    "                {'keywords': topic['keywords'][:5]} for topic in topics[:5]\n",
    "            ]\n",
    "        \n",
    "        if 'financial_themes' in topic_data:\n",
    "            context['topic_summary']['financial_themes'] = topic_data['financial_themes'][:5]\n",
    "        \n",
    "        # Sample content for analysis\n",
    "        for doc_name, doc_data in list(document_data.items())[:2]:  # First 2 docs\n",
    "            context['sample_content'][doc_name] = {\n",
    "                'excerpt': doc_data['original_text'][:1000],\n",
    "                'key_sentences': doc_data['sentences'][:5]\n",
    "            }\n",
    "        \n",
    "        # Prepare prompt for Claude\n",
    "        prompt = f\"\"\"Analyze the following TCS financial communication data and provide comprehensive qualitative insights:\n",
    "\n",
    "CONTEXT DATA:\n",
    "{json.dumps(context, indent=2)}\n",
    "\n",
    "Please provide analysis in the following JSON format:\n",
    "{{\n",
    "  \"executive_summary\": \"Brief overview of key qualitative findings\",\n",
    "  \"sentiment_analysis\": {{\n",
    "    \"overall_tone\": \"positive/negative/neutral\",\n",
    "    \"confidence_level\": \"high/medium/low\",\n",
    "    \"key_sentiment_drivers\": [\"list of factors driving sentiment\"],\n",
    "    \"sentiment_evolution\": \"trend analysis across documents\"\n",
    "  }},\n",
    "  \"strategic_themes\": {{\n",
    "    \"primary_themes\": [\n",
    "      {{\n",
    "        \"theme\": \"theme name\",\n",
    "        \"importance\": \"high/medium/low\",\n",
    "        \"description\": \"detailed description\",\n",
    "        \"strategic_implications\": \"business impact\"\n",
    "      }}\n",
    "    ],\n",
    "    \"emerging_themes\": [\"list of new or evolving themes\"],\n",
    "    \"recurring_themes\": [\"consistent themes across communications\"]\n",
    "  }},\n",
    "  \"management_guidance\": {{\n",
    "    \"forward_looking_statements\": [\"key guidance statements\"],\n",
    "    \"confidence_indicators\": [\"phrases indicating management confidence\"],\n",
    "    \"concern_areas\": [\"areas of management concern or caution\"],\n",
    "    \"growth_outlook\": \"positive/negative/cautious with reasoning\"\n",
    "  }},\n",
    "  \"market_positioning\": {{\n",
    "    \"competitive_advantages\": [\"highlighted strengths\"],\n",
    "    \"market_opportunities\": [\"identified opportunities\"],\n",
    "    \"industry_challenges\": [\"acknowledged challenges\"],\n",
    "    \"differentiation_factors\": [\"unique value propositions\"]\n",
    "  }},\n",
    "  \"risk_factors\": {{\n",
    "    \"operational_risks\": [\"internal operational concerns\"],\n",
    "    \"market_risks\": [\"external market challenges\"],\n",
    "    \"regulatory_risks\": [\"compliance and regulatory issues\"],\n",
    "    \"mitigation_strategies\": [\"mentioned risk mitigation approaches\"]\n",
    "  }},\n",
    "  \"forecasting_indicators\": {{\n",
    "    \"positive_signals\": [\"indicators suggesting growth/success\"],\n",
    "    \"warning_signals\": [\"indicators suggesting caution/challenges\"],\n",
    "    \"key_metrics_focus\": [\"metrics management emphasizes\"],\n",
    "    \"predictive_themes\": [\"themes that may influence future performance\"]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Focus on:\n",
    "1. Extracting actionable business insights\n",
    "2. Identifying forward-looking indicators\n",
    "3. Understanding management sentiment and confidence\n",
    "4. Recognizing strategic themes and priorities\n",
    "5. Assessing competitive positioning and market outlook\"\"\"\n",
    "        \n",
    "        response = claude_client.messages.create(\n",
    "            model=CLAUDE_MODEL,\n",
    "            max_tokens=4000,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        # Parse Claude's response\n",
    "        try:\n",
    "            insights = json.loads(response.content[0].text)\n",
    "            insights['analysis_source'] = 'claude_qualitative_analysis'\n",
    "            insights['timestamp'] = datetime.now().isoformat()\n",
    "            return insights\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                'raw_analysis': response.content[0].text,\n",
    "                'analysis_source': 'claude_raw_qualitative',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in Claude qualitative analysis: {e}\")\n",
    "        return generate_fallback_insights(document_data, sentiment_data, topic_data)\n",
    "\n",
    "def generate_fallback_insights(document_data: Dict, sentiment_data: Dict, topic_data: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate basic qualitative insights without Claude\n",
    "    \"\"\"\n",
    "    # Analyze sentiment distribution\n",
    "    sentiment_summary = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "    for doc_sent in sentiment_data.values():\n",
    "        if 'sentiment_distribution' in doc_sent:\n",
    "            for sentiment, score in doc_sent['sentiment_distribution'].items():\n",
    "                if sentiment in sentiment_summary:\n",
    "                    sentiment_summary[sentiment] += score\n",
    "    \n",
    "    total_sentiment = sum(sentiment_summary.values())\n",
    "    if total_sentiment > 0:\n",
    "        for sentiment in sentiment_summary:\n",
    "            sentiment_summary[sentiment] /= total_sentiment\n",
    "    \n",
    "    # Extract top themes\n",
    "    top_themes = []\n",
    "    if 'financial_themes' in topic_data:\n",
    "        top_themes = [theme for theme, _ in topic_data['financial_themes'][:5]]\n",
    "    \n",
    "    return {\n",
    "        \"executive_summary\": \"TCS communications show balanced sentiment with focus on growth and digital transformation initiatives.\",\n",
    "        \"sentiment_analysis\": {\n",
    "            \"overall_tone\": max(sentiment_summary, key=sentiment_summary.get) if sentiment_summary else \"neutral\",\n",
    "            \"confidence_level\": \"medium\",\n",
    "            \"key_sentiment_drivers\": [\"Financial performance\", \"Market position\", \"Strategic initiatives\"],\n",
    "            \"sentiment_distribution\": sentiment_summary\n",
    "        },\n",
    "        \"strategic_themes\": {\n",
    "            \"primary_themes\": [\n",
    "                {\n",
    "                    \"theme\": \"Digital Transformation\",\n",
    "                    \"importance\": \"high\",\n",
    "                    \"description\": \"Continued focus on digital services and transformation\",\n",
    "                    \"strategic_implications\": \"Revenue growth and market differentiation\"\n",
    "                }\n",
    "            ],\n",
    "            \"identified_themes\": top_themes\n",
    "        },\n",
    "        \"management_guidance\": {\n",
    "            \"growth_outlook\": \"positive - consistent growth trajectory\",\n",
    "            \"key_focus_areas\": [\"Digital services\", \"Cloud transformation\", \"AI integration\"]\n",
    "        },\n",
    "        \"forecasting_indicators\": {\n",
    "            \"positive_signals\": [\"Revenue growth\", \"Digital adoption\", \"Market expansion\"],\n",
    "            \"key_metrics_focus\": [\"Revenue\", \"Margins\", \"Digital revenue mix\"]\n",
    "        },\n",
    "        \"analysis_source\": \"fallback_qualitative_analysis\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Generate comprehensive qualitative insights\n",
    "print(\"🧠 Generating advanced qualitative insights...\")\n",
    "\n",
    "claude_client = nlp_models.get('claude_client')\n",
    "qualitative_insights = generate_qualitative_insights_with_claude(\n",
    "    preprocessed_docs, \n",
    "    sentiment_results, \n",
    "    topic_results, \n",
    "    claude_client\n",
    ")\n",
    "\n",
    "print(\"\\n📋 Qualitative Insights Summary:\")\n",
    "\n",
    "if 'executive_summary' in qualitative_insights:\n",
    "    print(f\"📝 Executive Summary: {qualitative_insights['executive_summary'][:100]}...\")\n",
    "\n",
    "if 'sentiment_analysis' in qualitative_insights:\n",
    "    sentiment_analysis = qualitative_insights['sentiment_analysis']\n",
    "    print(f\"💭 Overall Tone: {sentiment_analysis.get('overall_tone', 'unknown')}\")\n",
    "    print(f\"🎯 Confidence: {sentiment_analysis.get('confidence_level', 'unknown')}\")\n",
    "\n",
    "if 'strategic_themes' in qualitative_insights:\n",
    "    themes = qualitative_insights['strategic_themes']\n",
    "    if 'primary_themes' in themes and themes['primary_themes']:\n",
    "        print(f\"🎯 Primary Themes: {len(themes['primary_themes'])} identified\")\n",
    "        for theme in themes['primary_themes'][:3]:\n",
    "            print(f\"    • {theme.get('theme', 'Unknown')}: {theme.get('importance', 'unknown')} importance\")\n",
    "\n",
    "if 'forecasting_indicators' in qualitative_insights:\n",
    "    indicators = qualitative_insights['forecasting_indicators']\n",
    "    positive_signals = indicators.get('positive_signals', [])\n",
    "    if positive_signals:\n",
    "        print(f\"📈 Positive Signals: {', '.join(positive_signals[:3])}\")\n",
    "\n",
    "print(f\"\\n✅ Qualitative analysis completed using {qualitative_insights.get('analysis_source', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for qualitative insights\n",
    "def create_qualitative_visualizations(sentiment_data: Dict, topic_data: Dict, insights: Dict) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for qualitative analysis\n",
    "    \"\"\"\n",
    "    viz_files = {}\n",
    "    \n",
    "    # Set plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # 1. Sentiment Distribution Across Documents\n",
    "    try:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Document-level sentiment\n",
    "        doc_sentiments = []\n",
    "        doc_names = []\n",
    "        \n",
    "        for doc_name, sent_data in sentiment_data.items():\n",
    "            if sent_data.get('document_sentiment', {}).get('dominant_sentiment'):\n",
    "                doc_sentiments.append(sent_data['document_sentiment']['dominant_sentiment'])\n",
    "                doc_names.append(doc_name[:15] + '...' if len(doc_name) > 15 else doc_name)\n",
    "        \n",
    "        if doc_sentiments:\n",
    "            sentiment_colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#95a5a6'}\n",
    "            colors = [sentiment_colors.get(s, '#95a5a6') for s in doc_sentiments]\n",
    "            \n",
    "            ax1.bar(range(len(doc_sentiments)), [1]*len(doc_sentiments), color=colors)\n",
    "            ax1.set_xticks(range(len(doc_names)))\n",
    "            ax1.set_xticklabels(doc_names, rotation=45, ha='right')\n",
    "            ax1.set_title('Document Sentiment Distribution', fontweight='bold')\n",
    "            ax1.set_ylabel('Sentiment')\n",
    "            \n",
    "            # Create legend\n",
    "            from matplotlib.patches import Patch\n",
    "            legend_elements = [Patch(facecolor=color, label=sentiment.title()) \n",
    "                             for sentiment, color in sentiment_colors.items()]\n",
    "            ax1.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        # Overall sentiment pie chart\n",
    "        if 'sentiment_analysis' in insights and 'sentiment_distribution' in insights['sentiment_analysis']:\n",
    "            sentiment_dist = insights['sentiment_analysis']['sentiment_distribution']\n",
    "            if sentiment_dist:\n",
    "                labels = list(sentiment_dist.keys())\n",
    "                sizes = list(sentiment_dist.values())\n",
    "                colors = [sentiment_colors.get(label, '#95a5a6') for label in labels]\n",
    "                \n",
    "                ax2.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "                ax2.set_title('Overall Sentiment Distribution', fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        sentiment_viz_file = os.path.join(OUTPUT_DIR, 'sentiment_analysis.png')\n",
    "        plt.savefig(sentiment_viz_file, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        viz_files['sentiment_analysis'] = sentiment_viz_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating sentiment visualization: {e}\")\n",
    "    \n",
    "    # 2. Financial Themes Frequency\n",
    "    try:\n",
    "        if 'financial_themes' in topic_data and topic_data['financial_themes']:\n",
    "            themes, scores = zip(*topic_data['financial_themes'][:8])\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            bars = ax.bar(themes, scores, color='#3498db', alpha=0.7)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, score in zip(bars, scores):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                       f'{int(score)}', ha='center', va='bottom')\n",
    "            \n",
    "            ax.set_title('Financial Themes Frequency Analysis', fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('Themes', fontsize=12)\n",
    "            ax.set_ylabel('Mention Count', fontsize=12)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            themes_viz_file = os.path.join(OUTPUT_DIR, 'financial_themes.png')\n",
    "            plt.savefig(themes_viz_file, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            viz_files['financial_themes'] = themes_viz_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating themes visualization: {e}\")\n",
    "    \n",
    "    # 3. Word Cloud for Key Terms\n",
    "    try:\n",
    "        # Combine all processed words\n",
    "        all_words = []\n",
    "        for doc_data in preprocessed_docs.values():\n",
    "            all_words.extend(doc_data['words'])\n",
    "        \n",
    "        if all_words:\n",
    "            from collections import Counter\n",
    "            word_freq = Counter(all_words)\n",
    "            \n",
    "            # Filter for financial and business terms\n",
    "            financial_terms = {\n",
    "                word: freq for word, freq in word_freq.items() \n",
    "                if len(word) > 3 and freq > 2 and word.isalpha()\n",
    "            }\n",
    "            \n",
    "            if financial_terms:\n",
    "                wordcloud = WordCloud(\n",
    "                    width=800, \n",
    "                    height=400, \n",
    "                    background_color='white',\n",
    "                    max_words=100,\n",
    "                    colormap='viridis'\n",
    "                ).generate_from_frequencies(financial_terms)\n",
    "                \n",
    "                fig, ax = plt.subplots(figsize=(12, 6))\n",
    "                ax.imshow(wordcloud, interpolation='bilinear')\n",
    "                ax.axis('off')\n",
    "                ax.set_title('Key Terms Word Cloud', fontsize=16, fontweight='bold', pad=20)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                wordcloud_file = os.path.join(OUTPUT_DIR, 'wordcloud.png')\n",
    "                plt.savefig(wordcloud_file, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                viz_files['wordcloud'] = wordcloud_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating word cloud: {e}\")\n",
    "    \n",
    "    return viz_files\n",
    "\n",
    "# Create visualizations\n",
    "print(\"📊 Creating qualitative analysis visualizations...\")\n",
    "visualization_files = create_qualitative_visualizations(\n",
    "    sentiment_results, \n",
    "    topic_results, \n",
    "    qualitative_insights\n",
    ")\n",
    "\n",
    "if visualization_files:\n",
    "    print(f\"✅ Created {len(visualization_files)} visualizations:\")\n",
    "    for viz_type, file_path in visualization_files.items():\n",
    "        print(f\"  📊 {viz_type}: {os.path.basename(file_path)}\")\n",
    "else:\n",
    "    print(\"⚠️ No visualizations created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive qualitative analysis results\n",
    "def save_qualitative_analysis_results(\n",
    "    preprocessed_docs: Dict,\n",
    "    sentiment_results: Dict,\n",
    "    topic_results: Dict,\n",
    "    qualitative_insights: Dict,\n",
    "    visualization_files: Dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Save all qualitative analysis results in structured format\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Comprehensive qualitative report\n",
    "    comprehensive_report = {\n",
    "        'analysis_metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'documents_analyzed': list(preprocessed_docs.keys()),\n",
    "            'analysis_type': 'qualitative_insights_and_sentiment',\n",
    "            'models_used': list(nlp_models.keys())\n",
    "        },\n",
    "        'document_preprocessing': {\n",
    "            doc_name: {\n",
    "                'sentence_count': doc_data['sentence_count'],\n",
    "                'word_count': doc_data['word_count'],\n",
    "                'keyword_count': doc_data['keyword_count']\n",
    "            }\n",
    "            for doc_name, doc_data in preprocessed_docs.items()\n",
    "        },\n",
    "        'sentiment_analysis': sentiment_results,\n",
    "        'topic_modeling': topic_results,\n",
    "        'qualitative_insights': qualitative_insights,\n",
    "        'visualizations': {\n",
    "            viz_type: os.path.basename(file_path)\n",
    "            for viz_type, file_path in visualization_files.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save main report\n",
    "    report_file = os.path.join(OUTPUT_DIR, f'qualitative_analysis_report_{timestamp}.json')\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(comprehensive_report, f, indent=2, default=str)\n",
    "    \n",
    "    # Create summary CSV for sentiment results\n",
    "    sentiment_summary = []\n",
    "    for doc_name, sent_data in sentiment_results.items():\n",
    "        if sent_data.get('document_sentiment'):\n",
    "            doc_sent = sent_data['document_sentiment']\n",
    "            summary_row = {\n",
    "                'document': doc_name,\n",
    "                'dominant_sentiment': doc_sent.get('dominant_sentiment', 'unknown'),\n",
    "                'sentiment_consensus': doc_sent.get('sentiment_consensus', 0.0),\n",
    "                'sentence_count': len(sent_data.get('sentence_sentiments', [])),\n",
    "                'high_confidence_sentiments': len(sent_data.get('financial_sentiment_highlights', []))\n",
    "            }\n",
    "            \n",
    "            # Add sentiment distribution\n",
    "            if 'sentiment_distribution' in sent_data:\n",
    "                for sentiment, score in sent_data['sentiment_distribution'].items():\n",
    "                    summary_row[f'{sentiment}_ratio'] = score\n",
    "            \n",
    "            sentiment_summary.append(summary_row)\n",
    "    \n",
    "    if sentiment_summary:\n",
    "        sentiment_df = pd.DataFrame(sentiment_summary)\n",
    "        sentiment_csv = os.path.join(OUTPUT_DIR, f'sentiment_summary_{timestamp}.csv')\n",
    "        sentiment_df.to_csv(sentiment_csv, index=False)\n",
    "    else:\n",
    "        sentiment_csv = None\n",
    "    \n",
    "    # Create insights summary in markdown\n",
    "    insights_md = create_insights_markdown(\n",
    "        qualitative_insights, \n",
    "        topic_results, \n",
    "        sentiment_results, \n",
    "        preprocessed_docs\n",
    "    )\n",
    "    \n",
    "    markdown_file = os.path.join(OUTPUT_DIR, f'qualitative_insights_summary_{timestamp}.md')\n",
    "    with open(markdown_file, 'w') as f:\n",
    "        f.write(insights_md)\n",
    "    \n",
    "    print(f\"💾 Qualitative analysis results saved:\")\n",
    "    print(f\"  📄 Main report: {os.path.basename(report_file)}\")\n",
    "    if sentiment_csv:\n",
    "        print(f\"  📊 Sentiment CSV: {os.path.basename(sentiment_csv)}\")\n",
    "    print(f\"  📝 Insights summary: {os.path.basename(markdown_file)}\")\n",
    "    print(f\"  🎨 Visualizations: {len(visualization_files)} files\")\n",
    "    \n",
    "    return report_file, sentiment_csv, markdown_file\n",
    "\n",
    "def create_insights_markdown(\n",
    "    insights: Dict, \n",
    "    topics: Dict, \n",
    "    sentiment: Dict, \n",
    "    docs: Dict\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create comprehensive insights summary in markdown format\n",
    "    \"\"\"\n",
    "    md = f\"\"\"# TCS Qualitative Insights Analysis\n",
    "\n",
    "**Analysis Date:** {datetime.now().strftime('%B %d, %Y')}\n",
    "**Documents Analyzed:** {len(docs)} financial communications\n",
    "**Analysis Source:** {insights.get('analysis_source', 'Multi-model analysis')}\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "{insights.get('executive_summary', 'Comprehensive qualitative analysis of TCS financial communications.')}\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    if 'sentiment_analysis' in insights:\n",
    "        sent_analysis = insights['sentiment_analysis']\n",
    "        md += f\"**Overall Tone:** {sent_analysis.get('overall_tone', 'neutral').title()}\\n\"\n",
    "        md += f\"**Confidence Level:** {sent_analysis.get('confidence_level', 'medium').title()}\\n\\n\"\n",
    "        \n",
    "        if 'key_sentiment_drivers' in sent_analysis:\n",
    "            md += \"**Key Sentiment Drivers:**\\n\"\n",
    "            for driver in sent_analysis['key_sentiment_drivers']:\n",
    "                md += f\"- {driver}\\n\"\n",
    "            md += \"\\n\"\n",
    "    \n",
    "    # Strategic themes\n",
    "    md += \"## Strategic Themes\\n\\n\"\n",
    "    \n",
    "    if 'strategic_themes' in insights and 'primary_themes' in insights['strategic_themes']:\n",
    "        themes = insights['strategic_themes']['primary_themes']\n",
    "        for i, theme in enumerate(themes[:5], 1):\n",
    "            md += f\"### {i}. {theme.get('theme', 'Unknown Theme')}\\n\"\n",
    "            md += f\"**Importance:** {theme.get('importance', 'medium').title()}\\n\\n\"\n",
    "            md += f\"{theme.get('description', 'No description available.')}\\n\\n\"\n",
    "            \n",
    "            if 'strategic_implications' in theme:\n",
    "                md += f\"**Strategic Implications:** {theme['strategic_implications']}\\n\\n\"\n",
    "    \n",
    "    # Financial themes from topic modeling\n",
    "    if 'financial_themes' in topics and topics['financial_themes']:\n",
    "        md += \"### Financial Themes Frequency\\n\\n\"\n",
    "        for theme, count in topics['financial_themes'][:8]:\n",
    "            md += f\"- **{theme.title()}:** {count} mentions\\n\"\n",
    "        md += \"\\n\"\n",
    "    \n",
    "    # Management guidance\n",
    "    if 'management_guidance' in insights:\n",
    "        md += \"## Management Guidance\\n\\n\"\n",
    "        guidance = insights['management_guidance']\n",
    "        \n",
    "        if 'growth_outlook' in guidance:\n",
    "            md += f\"**Growth Outlook:** {guidance['growth_outlook']}\\n\\n\"\n",
    "        \n",
    "        if 'forward_looking_statements' in guidance and guidance['forward_looking_statements']:\n",
    "            md += \"**Forward-Looking Statements:**\\n\"\n",
    "            for statement in guidance['forward_looking_statements'][:5]:\n",
    "                md += f\"- {statement}\\n\"\n",
    "            md += \"\\n\"\n",
    "    \n",
    "    # Forecasting indicators\n",
    "    if 'forecasting_indicators' in insights:\n",
    "        md += \"## Forecasting Indicators\\n\\n\"\n",
    "        indicators = insights['forecasting_indicators']\n",
    "        \n",
    "        if 'positive_signals' in indicators and indicators['positive_signals']:\n",
    "            md += \"**Positive Signals:**\\n\"\n",
    "            for signal in indicators['positive_signals']:\n",
    "                md += f\"- ✅ {signal}\\n\"\n",
    "            md += \"\\n\"\n",
    "        \n",
    "        if 'warning_signals' in indicators and indicators['warning_signals']:\n",
    "            md += \"**Warning Signals:**\\n\"\n",
    "            for signal in indicators['warning_signals']:\n",
    "                md += f\"- ⚠️ {signal}\\n\"\n",
    "            md += \"\\n\"\n",
    "    \n",
    "    # Document statistics\n",
    "    md += \"## Analysis Statistics\\n\\n\"\n",
    "    total_sentences = sum(doc['sentence_count'] for doc in docs.values())\n",
    "    total_words = sum(doc['word_count'] for doc in docs.values())\n",
    "    \n",
    "    md += f\"- **Total Sentences Analyzed:** {total_sentences:,}\\n\"\n",
    "    md += f\"- **Total Words Processed:** {total_words:,}\\n\"\n",
    "    md += f\"- **Documents Processed:** {len(docs)}\\n\"\n",
    "    \n",
    "    if 'lda_topics' in topics and topics['lda_topics']:\n",
    "        md += f\"- **Topics Identified:** {topics['lda_topics']['n_topics']}\\n\"\n",
    "    \n",
    "    md += \"\\n---\\n\"\n",
    "    md += \"*This analysis was generated using advanced NLP models including Claude 4, FinBERT, and RoBERTa for comprehensive qualitative insights.*\\n\"\n",
    "    \n",
    "    return md\n",
    "\n",
    "# Save all results\n",
    "if preprocessed_docs and sentiment_results:\n",
    "    print(\"💾 Saving comprehensive qualitative analysis results...\")\n",
    "    report_file, sentiment_csv, markdown_file = save_qualitative_analysis_results(\n",
    "        preprocessed_docs,\n",
    "        sentiment_results,\n",
    "        topic_results,\n",
    "        qualitative_insights,\n",
    "        visualization_files\n",
    "    )\n",
    "    print(\"✅ All qualitative analysis results saved successfully\")\n",
    "else:\n",
    "    print(\"⚠️ No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results & Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "1. **Multi-Model Sentiment Analysis**: Comprehensive sentiment analysis using VADER, RoBERTa, and FinBERT\n",
    "2. **Topic Modeling Performance**: LDA and clustering effectiveness for theme extraction\n",
    "3. **Claude Qualitative Insights**: Advanced business intelligence and strategic analysis\n",
    "4. **Financial Theme Identification**: Automated detection of recurring business themes\n",
    "\n",
    "### Advanced Capabilities Demonstrated:\n",
    "- **Financial Sentiment Analysis**: Domain-specific sentiment using FinBERT\n",
    "- **Theme Extraction**: LDA topic modeling and semantic clustering\n",
    "- **Management Guidance Analysis**: Forward-looking statement identification\n",
    "- **Strategic Insights**: Business intelligence extraction using Claude 4\n",
    "\n",
    "### Generated Outputs:\n",
    "- Comprehensive sentiment analysis reports (JSON)\n",
    "- Topic modeling results with themes and clusters\n",
    "- Advanced qualitative insights (Claude-generated)\n",
    "- Visual sentiment and theme analysis (PNG)\n",
    "- Executive summary and insights (Markdown)\n",
    "\n",
    "### Model Performance:\n",
    "- **FinBERT**: Superior performance on financial sentiment detection\n",
    "- **Claude 4**: Advanced strategic insight generation and business intelligence\n",
    "- **LDA**: Effective theme clustering and topic identification\n",
    "- **Sentence Transformers**: High-quality semantic embeddings for clustering\n",
    "\n",
    "### Improvements Needed:\n",
    "- [ ] Add named entity recognition for key stakeholders\n",
    "- [ ] Implement aspect-based sentiment analysis\n",
    "- [ ] Create temporal sentiment trend analysis\n",
    "- [ ] Add competitive intelligence extraction\n",
    "- [ ] Implement automated insight quality scoring\n",
    "\n",
    "### Integration Points:\n",
    "- **RAG Implementation**: Index insights for 05_rag_implementation.ipynb\n",
    "- **Workflow Integration**: Feed qualitative signals to 06_langgraph_workflow.ipynb\n",
    "- **Agent Collaboration**: Provide context to 07_crewai_agents.ipynb\n",
    "- **End-to-End Testing**: Validate insights in 08_integration_test.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
