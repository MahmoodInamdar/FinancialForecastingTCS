{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8: Integration Testing\n",
    "\n",
    "## Overview\n",
    "This notebook provides end-to-end integration testing of all experimental components:\n",
    "- Document discovery and loading\n",
    "- Table extraction and processing\n",
    "- Financial analysis and insights\n",
    "- RAG implementation\n",
    "- LangGraph workflow orchestration\n",
    "- CrewAI multi-agent collaboration\n",
    "\n",
    "## Objectives\n",
    "1. Test complete workflow integration\n",
    "2. Validate data flow between components\n",
    "3. Measure end-to-end performance\n",
    "4. Identify integration bottlenecks\n",
    "5. Generate comprehensive test reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(\"Integration Test Environment Initialized\")\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Current Time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Configuration\n",
    "TEST_CONFIG = {\n",
    "    \"data_folder\": project_root / \"data\",\n",
    "    \"output_folder\": project_root / \"experiments\" / \"integration_outputs\",\n",
    "    \"test_company\": \"TCS\",\n",
    "    \"test_files\": [\n",
    "        \"sample_financial_report.pdf\",\n",
    "        \"market_data.xlsx\"\n",
    "    ],\n",
    "    \"performance_thresholds\": {\n",
    "        \"document_discovery\": 30,  # seconds\n",
    "        \"table_extraction\": 60,    # seconds\n",
    "        \"financial_analysis\": 45,  # seconds\n",
    "        \"rag_retrieval\": 15,       # seconds\n",
    "        \"workflow_execution\": 180, # seconds\n",
    "        \"agent_collaboration\": 120 # seconds\n",
    "    },\n",
    "    \"quality_thresholds\": {\n",
    "        \"extraction_accuracy\": 0.85,\n",
    "        \"analysis_completeness\": 0.90,\n",
    "        \"insight_relevance\": 0.80\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "TEST_CONFIG[\"output_folder\"].mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Test Configuration:\")\n",
    "for key, value in TEST_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegrationTestSuite:\n",
    "    \"\"\"Comprehensive integration test suite for financial forecasting agent\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.test_results = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"tests\": {},\n",
    "            \"performance\": {},\n",
    "            \"quality_metrics\": {},\n",
    "            \"errors\": [],\n",
    "            \"summary\": {}\n",
    "        }\n",
    "        \n",
    "    def log_test_start(self, test_name: str):\n",
    "        \"\"\"Log test start and initialize timing\"\"\"\n",
    "        logger.info(f\"Starting test: {test_name}\")\n",
    "        self.test_results[\"tests\"][test_name] = {\n",
    "            \"start_time\": time.time(),\n",
    "            \"status\": \"running\"\n",
    "        }\n",
    "        \n",
    "    def log_test_end(self, test_name: str, status: str, details: Dict = None):\n",
    "        \"\"\"Log test completion with results\"\"\"\n",
    "        end_time = time.time()\n",
    "        duration = end_time - self.test_results[\"tests\"][test_name][\"start_time\"]\n",
    "        \n",
    "        self.test_results[\"tests\"][test_name].update({\n",
    "            \"end_time\": end_time,\n",
    "            \"duration\": duration,\n",
    "            \"status\": status,\n",
    "            \"details\": details or {}\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Completed test: {test_name} - Status: {status} - Duration: {duration:.2f}s\")\n",
    "        \n",
    "    def log_error(self, test_name: str, error: Exception):\n",
    "        \"\"\"Log test error\"\"\"\n",
    "        error_info = {\n",
    "            \"test\": test_name,\n",
    "            \"error_type\": type(error).__name__,\n",
    "            \"error_message\": str(error),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        self.test_results[\"errors\"].append(error_info)\n",
    "        logger.error(f\"Test {test_name} failed: {error}\")\n",
    "        \n",
    "    def save_results(self):\n",
    "        \"\"\"Save test results to file\"\"\"\n",
    "        output_file = self.config[\"output_folder\"] / f\"integration_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(self.test_results, f, indent=2, default=str)\n",
    "            \n",
    "        logger.info(f\"Test results saved to: {output_file}\")\n",
    "        return output_file\n",
    "\n",
    "# Initialize test suite\n",
    "test_suite = IntegrationTestSuite(TEST_CONFIG)\n",
    "print(\"Integration Test Suite Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Document Discovery Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_document_discovery():\n",
    "    \"\"\"Test document discovery and loading functionality\"\"\"\n",
    "    test_name = \"document_discovery\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Import document discovery components\n",
    "        from experiments.experiment_01_document_discovery import (\n",
    "            DocumentDiscovery, DocumentClassifier, MetadataExtractor\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        discovery = DocumentDiscovery()\n",
    "        classifier = DocumentClassifier()\n",
    "        extractor = MetadataExtractor()\n",
    "        \n",
    "        # Test document loading from data folder\n",
    "        data_folder = TEST_CONFIG[\"data_folder\"]\n",
    "        documents = discovery.load_local_documents(data_folder)\n",
    "        \n",
    "        # Test document classification\n",
    "        classified_docs = []\n",
    "        for doc in documents:\n",
    "            doc_type = classifier.classify_document(doc)\n",
    "            metadata = extractor.extract_metadata(doc)\n",
    "            classified_docs.append({\n",
    "                \"document\": doc,\n",
    "                \"type\": doc_type,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        # Validate results\n",
    "        assert len(documents) > 0, \"No documents found in data folder\"\n",
    "        assert len(classified_docs) == len(documents), \"Document classification failed\"\n",
    "        \n",
    "        details = {\n",
    "            \"documents_found\": len(documents),\n",
    "            \"classifications\": [doc[\"type\"] for doc in classified_docs],\n",
    "            \"metadata_extracted\": len([doc for doc in classified_docs if doc[\"metadata\"]])\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"passed\", details)\n",
    "        return classified_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return []\n",
    "\n",
    "# Run document discovery test\n",
    "discovered_documents = await test_document_discovery()\n",
    "print(f\"Document Discovery Test Completed - Found {len(discovered_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Table Extraction Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_table_extraction(documents: List[Dict]):\n",
    "    \"\"\"Test table extraction from discovered documents\"\"\"\n",
    "    test_name = \"table_extraction\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Import table extraction components\n",
    "        from experiments.experiment_02_table_extraction import (\n",
    "            MultiModelTableExtractor, TableProcessor, ValidationFramework\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        extractor = MultiModelTableExtractor()\n",
    "        processor = TableProcessor()\n",
    "        validator = ValidationFramework()\n",
    "        \n",
    "        extracted_tables = []\n",
    "        \n",
    "        # Process PDF documents for table extraction\n",
    "        pdf_documents = [doc for doc in documents if doc[\"document\"].endswith(\".pdf\")]\n",
    "        \n",
    "        for doc_info in pdf_documents[:2]:  # Limit to first 2 PDFs for testing\n",
    "            doc_path = doc_info[\"document\"]\n",
    "            \n",
    "            # Extract tables using multiple models\n",
    "            qwen_tables = extractor.extract_with_qwen(doc_path)\n",
    "            layoutlm_tables = extractor.extract_with_layoutlm(doc_path)\n",
    "            kosmos_tables = extractor.extract_with_kosmos(doc_path)\n",
    "            \n",
    "            # Process and validate results\n",
    "            all_tables = {\n",
    "                \"qwen\": qwen_tables,\n",
    "                \"layoutlm\": layoutlm_tables,\n",
    "                \"kosmos\": kosmos_tables\n",
    "            }\n",
    "            \n",
    "            # Select best results\n",
    "            best_tables = validator.select_best_results(all_tables)\n",
    "            processed_tables = processor.process_tables(best_tables)\n",
    "            \n",
    "            extracted_tables.append({\n",
    "                \"document\": doc_path,\n",
    "                \"raw_tables\": all_tables,\n",
    "                \"best_tables\": best_tables,\n",
    "                \"processed_tables\": processed_tables\n",
    "            })\n",
    "        \n",
    "        # Validate results\n",
    "        total_tables = sum(len(doc[\"processed_tables\"]) for doc in extracted_tables)\n",
    "        assert total_tables > 0, \"No tables extracted from documents\"\n",
    "        \n",
    "        details = {\n",
    "            \"documents_processed\": len(extracted_tables),\n",
    "            \"total_tables_extracted\": total_tables,\n",
    "            \"extraction_methods_used\": [\"qwen\", \"layoutlm\", \"kosmos\"],\n",
    "            \"average_tables_per_doc\": total_tables / len(extracted_tables) if extracted_tables else 0\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"passed\", details)\n",
    "        return extracted_tables\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return []\n",
    "\n",
    "# Run table extraction test\n",
    "extracted_tables = await test_table_extraction(discovered_documents)\n",
    "print(f\"Table Extraction Test Completed - Processed {len(extracted_tables)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Financial Analysis Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_financial_analysis(tables_data: List[Dict]):\n",
    "    \"\"\"Test financial analysis on extracted table data\"\"\"\n",
    "    test_name = \"financial_analysis\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Import financial analysis components\n",
    "        from experiments.experiment_03_financial_analysis import (\n",
    "            ClaudeFinancialAnalyzer, MetricsCalculator, TrendAnalyzer\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        analyzer = ClaudeFinancialAnalyzer()\n",
    "        calculator = MetricsCalculator()\n",
    "        trend_analyzer = TrendAnalyzer()\n",
    "        \n",
    "        analysis_results = []\n",
    "        \n",
    "        # Analyze extracted financial data\n",
    "        for table_doc in tables_data:\n",
    "            document = table_doc[\"document\"]\n",
    "            processed_tables = table_doc[\"processed_tables\"]\n",
    "            \n",
    "            # Calculate financial metrics\n",
    "            metrics = calculator.calculate_metrics(processed_tables)\n",
    "            \n",
    "            # Perform trend analysis\n",
    "            trends = trend_analyzer.analyze_trends(processed_tables)\n",
    "            \n",
    "            # Generate Claude analysis\n",
    "            claude_analysis = await analyzer.analyze_financial_data({\n",
    "                \"tables\": processed_tables,\n",
    "                \"metrics\": metrics,\n",
    "                \"trends\": trends\n",
    "            })\n",
    "            \n",
    "            analysis_results.append({\n",
    "                \"document\": document,\n",
    "                \"metrics\": metrics,\n",
    "                \"trends\": trends,\n",
    "                \"claude_analysis\": claude_analysis\n",
    "            })\n",
    "        \n",
    "        # Validate results\n",
    "        assert len(analysis_results) > 0, \"No financial analysis results generated\"\n",
    "        \n",
    "        # Check for completeness\n",
    "        complete_analyses = sum(1 for result in analysis_results \n",
    "                              if result[\"metrics\"] and result[\"trends\"] and result[\"claude_analysis\"])\n",
    "        \n",
    "        completeness_ratio = complete_analyses / len(analysis_results)\n",
    "        assert completeness_ratio >= TEST_CONFIG[\"quality_thresholds\"][\"analysis_completeness\"], \\\n",
    "               f\"Analysis completeness {completeness_ratio} below threshold\"\n",
    "        \n",
    "        details = {\n",
    "            \"documents_analyzed\": len(analysis_results),\n",
    "            \"complete_analyses\": complete_analyses,\n",
    "            \"completeness_ratio\": completeness_ratio,\n",
    "            \"metrics_calculated\": len([r for r in analysis_results if r[\"metrics\"]]),\n",
    "            \"trends_analyzed\": len([r for r in analysis_results if r[\"trends\"]])\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"passed\", details)\n",
    "        return analysis_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return []\n",
    "\n",
    "# Run financial analysis test\n",
    "financial_analyses = await test_financial_analysis(extracted_tables)\n",
    "print(f\"Financial Analysis Test Completed - Analyzed {len(financial_analyses)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: RAG Implementation Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_rag_implementation(analysis_data: List[Dict]):\n",
    "    \"\"\"Test RAG implementation for knowledge retrieval\"\"\"\n",
    "    test_name = \"rag_implementation\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Import RAG components\n",
    "        from experiments.experiment_05_rag_implementation import (\n",
    "            HyDERetriever, ContextualRAG, HybridSearch, VectorStore\n",
    "        )\n",
    "        \n",
    "        # Initialize components\n",
    "        hyde_retriever = HyDERetriever()\n",
    "        contextual_rag = ContextualRAG()\n",
    "        hybrid_search = HybridSearch()\n",
    "        vector_store = VectorStore()\n",
    "        \n",
    "        # Build knowledge base from analysis results\n",
    "        documents = []\n",
    "        for analysis in analysis_data:\n",
    "            # Extract text content for indexing\n",
    "            doc_content = {\n",
    "                \"document_id\": analysis[\"document\"],\n",
    "                \"metrics\": analysis[\"metrics\"],\n",
    "                \"trends\": analysis[\"trends\"],\n",
    "                \"analysis\": analysis[\"claude_analysis\"],\n",
    "                \"content\": f\"{analysis['claude_analysis']} {json.dumps(analysis['metrics'])} {json.dumps(analysis['trends'])}\"\n",
    "            }\n",
    "            documents.append(doc_content)\n",
    "        \n",
    "        # Index documents in vector store\n",
    "        indexing_results = vector_store.index_documents(documents)\n",
    "        \n",
    "        # Test query scenarios\n",
    "        test_queries = [\n",
    "            \"What are the key financial metrics for TCS?\",\n",
    "            \"Show me revenue trends and growth patterns\",\n",
    "            \"What are the main financial risks and opportunities?\",\n",
    "            \"Compare profitability ratios across different periods\"\n",
    "        ]\n",
    "        \n",
    "        query_results = []\n",
    "        for query in test_queries:\n",
    "            # Test HyDE retrieval\n",
    "            hyde_results = hyde_retriever.retrieve(query, documents)\n",
    "            \n",
    "            # Test Contextual RAG\n",
    "            contextual_results = await contextual_rag.query(query, documents)\n",
    "            \n",
    "            # Test Hybrid Search\n",
    "            hybrid_results = hybrid_search.search(query, vector_store)\n",
    "            \n",
    "            query_results.append({\n",
    "                \"query\": query,\n",
    "                \"hyde_results\": hyde_results,\n",
    "                \"contextual_results\": contextual_results,\n",
    "                \"hybrid_results\": hybrid_results\n",
    "            })\n",
    "        \n",
    "        # Validate results\n",
    "        assert len(query_results) == len(test_queries), \"Not all queries processed\"\n",
    "        \n",
    "        # Check retrieval quality\n",
    "        successful_retrievals = sum(1 for result in query_results \n",
    "                                  if result[\"hyde_results\"] and result[\"contextual_results\"] and result[\"hybrid_results\"])\n",
    "        \n",
    "        retrieval_success_rate = successful_retrievals / len(test_queries)\n",
    "        \n",
    "        details = {\n",
    "            \"documents_indexed\": len(documents),\n",
    "            \"indexing_success\": indexing_results[\"success\"] if indexing_results else False,\n",
    "            \"test_queries\": len(test_queries),\n",
    "            \"successful_retrievals\": successful_retrievals,\n",
    "            \"retrieval_success_rate\": retrieval_success_rate,\n",
    "            \"retrieval_methods\": [\"HyDE\", \"Contextual RAG\", \"Hybrid Search\"]\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"passed\", details)\n",
    "        return query_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return []\n",
    "\n",
    "# Run RAG implementation test\n",
    "rag_results = await test_rag_implementation(financial_analyses)\n",
    "print(f\"RAG Implementation Test Completed - Processed {len(rag_results)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: LangGraph Workflow Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_langgraph_workflow(documents: List[Dict], analyses: List[Dict]):\n",
    "    \"\"\"Test LangGraph workflow orchestration\"\"\"\n",
    "    test_name = \"langgraph_workflow\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Import LangGraph components\n",
    "        from experiments.experiment_06_langgraph_workflow import (\n",
    "            FinancialForecastWorkflow, FinancialForecastState, WorkflowNodes\n",
    "        )\n",
    "        \n",
    "        # Initialize workflow components\n",
    "        workflow = FinancialForecastWorkflow()\n",
    "        nodes = WorkflowNodes()\n",
    "        \n",
    "        # Create initial state\n",
    "        initial_state = FinancialForecastState(\n",
    "            company=\"TCS\",\n",
    "            documents=documents,\n",
    "            analyses=analyses,\n",
    "            current_step=\"initialization\",\n",
    "            results={},\n",
    "            errors=[]\n",
    "        )\n",
    "        \n",
    "        # Execute workflow\n",
    "        workflow_start_time = time.time()\n",
    "        final_state = await workflow.execute(initial_state)\n",
    "        workflow_duration = time.time() - workflow_start_time\n",
    "        \n",
    "        # Validate workflow execution\n",
    "        assert final_state is not None, \"Workflow execution failed\"\n",
    "        assert final_state.current_step == \"completed\", f\"Workflow not completed, stuck at: {final_state.current_step}\"\n",
    "        assert len(final_state.errors) == 0, f\"Workflow completed with errors: {final_state.errors}\"\n",
    "        \n",
    "        # Check performance threshold\n",
    "        performance_threshold = TEST_CONFIG[\"performance_thresholds\"][\"workflow_execution\"]\n",
    "        assert workflow_duration <= performance_threshold, \\\n",
    "               f\"Workflow execution time {workflow_duration:.2f}s exceeds threshold {performance_threshold}s\"\n",
    "        \n",
    "        # Validate workflow results\n",
    "        required_results = [\"document_processing\", \"financial_analysis\", \"insights\", \"forecast\"]\n",
    "        missing_results = [key for key in required_results if key not in final_state.results]\n",
    "        \n",
    "        assert len(missing_results) == 0, f\"Missing workflow results: {missing_results}\"\n",
    "        \n",
    "        details = {\n",
    "            \"workflow_duration\": workflow_duration,\n",
    "            \"final_step\": final_state.current_step,\n",
    "            \"error_count\": len(final_state.errors),\n",
    "            \"results_generated\": list(final_state.results.keys()),\n",
    "            \"performance_within_threshold\": workflow_duration <= performance_threshold,\n",
    "            \"workflow_nodes_executed\": len([step for step in final_state.execution_history if step.get(\"status\") == \"completed\"])\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"passed\", details)\n",
    "        return final_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return None\n",
    "\n",
    "# Run LangGraph workflow test\n",
    "workflow_results = await test_langgraph_workflow(discovered_documents, financial_analyses)\n",
    "print(f\"LangGraph Workflow Test Completed - Status: {'Passed' if workflow_results else 'Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: CrewAI Multi-Agent Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_crewai_agents(workflow_state: Any):\n",
    "    \"\"\"Test CrewAI multi-agent collaboration\"\"\"\n",
    "    test_name = \"crewai_agents\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Import CrewAI components\n",
    "        from experiments.experiment_07_crewai_agents import (\n",
    "            FinancialAnalystAgent, MarketResearchAgent, InsightsSpecialistAgent,\n",
    "            ForecastingExpertAgent, SynthesisManagerAgent, FinancialAnalysisCrew\n",
    "        )\n",
    "        \n",
    "        # Initialize agents\n",
    "        financial_analyst = FinancialAnalystAgent()\n",
    "        market_researcher = MarketResearchAgent()\n",
    "        insights_specialist = InsightsSpecialistAgent()\n",
    "        forecasting_expert = ForecastingExpertAgent()\n",
    "        synthesis_manager = SynthesisManagerAgent()\n",
    "        \n",
    "        # Create crew\n",
    "        crew = FinancialAnalysisCrew([\n",
    "            financial_analyst,\n",
    "            market_researcher,\n",
    "            insights_specialist,\n",
    "            forecasting_expert,\n",
    "            synthesis_manager\n",
    "        ])\n",
    "        \n",
    "        # Prepare task data from workflow results\n",
    "        task_data = {\n",
    "            \"company\": \"TCS\",\n",
    "            \"workflow_results\": workflow_state.results if workflow_state else {},\n",
    "            \"documents\": discovered_documents,\n",
    "            \"analyses\": financial_analyses\n",
    "        }\n",
    "        \n",
    "        # Execute multi-agent collaboration\n",
    "        collaboration_start_time = time.time()\n",
    "        crew_results = await crew.execute_collaborative_analysis(task_data)\n",
    "        collaboration_duration = time.time() - collaboration_start_time\n",
    "        \n",
    "        # Validate crew execution\n",
    "        assert crew_results is not None, \"CrewAI execution failed\"\n",
    "        assert \"final_report\" in crew_results, \"No final report generated\"\n",
    "        assert \"agent_contributions\" in crew_results, \"No agent contributions recorded\"\n",
    "        \n",
    "        # Check performance threshold\n",
    "        performance_threshold = TEST_CONFIG[\"performance_thresholds\"][\"agent_collaboration\"]\n",
    "        assert collaboration_duration <= performance_threshold, \\\n",
    "               f\"Agent collaboration time {collaboration_duration:.2f}s exceeds threshold {performance_threshold}s\"\n",
    "        \n",
    "        # Validate agent contributions\n",
    "        expected_agents = 5\n",
    "        actual_agents = len(crew_results[\"agent_contributions\"])\n",
    "        assert actual_agents == expected_agents, f\"Expected {expected_agents} agents, got {actual_agents}\"\n",
    "        \n",
    "        # Check task completion\n",
    "        completed_tasks = sum(1 for agent_result in crew_results[\"agent_contributions\"].values() \n",
    "                            if agent_result.get(\"status\") == \"completed\")\n",
    "        \n",
    "        task_completion_rate = completed_tasks / actual_agents\n",
    "        assert task_completion_rate >= 0.8, f\"Task completion rate {task_completion_rate} too low\"\n",
    "        \n",
    "        details = {\n",
    "            \"collaboration_duration\": collaboration_duration,\n",
    "            \"agents_participated\": actual_agents,\n",
    "            \"tasks_completed\": completed_tasks,\n",
    "            \"task_completion_rate\": task_completion_rate,\n",
    "            \"final_report_generated\": bool(crew_results.get(\"final_report\")),\n",
    "            \"performance_within_threshold\": collaboration_duration <= performance_threshold,\n",
    "            \"agent_types\": list(crew_results[\"agent_contributions\"].keys())\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"passed\", details)\n",
    "        return crew_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return None\n",
    "\n",
    "# Run CrewAI agents test\n",
    "crew_results = await test_crewai_agents(workflow_results)\n",
    "print(f\"CrewAI Agents Test Completed - Status: {'Passed' if crew_results else 'Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: End-to-End Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance():\n",
    "    \"\"\"Analyze overall performance metrics\"\"\"\n",
    "    test_name = \"performance_analysis\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Calculate total execution time\n",
    "        total_duration = sum(test[\"duration\"] for test in test_suite.test_results[\"tests\"].values() \n",
    "                           if \"duration\" in test)\n",
    "        \n",
    "        # Analyze individual component performance\n",
    "        performance_metrics = {}\n",
    "        for test_name, test_data in test_suite.test_results[\"tests\"].items():\n",
    "            if \"duration\" in test_data:\n",
    "                threshold = TEST_CONFIG[\"performance_thresholds\"].get(test_name, float('inf'))\n",
    "                performance_metrics[test_name] = {\n",
    "                    \"duration\": test_data[\"duration\"],\n",
    "                    \"threshold\": threshold,\n",
    "                    \"within_threshold\": test_data[\"duration\"] <= threshold,\n",
    "                    \"efficiency_ratio\": test_data[\"duration\"] / threshold if threshold != float('inf') else 1.0\n",
    "                }\n",
    "        \n",
    "        # Calculate overall efficiency\n",
    "        efficiency_scores = [metrics[\"efficiency_ratio\"] for metrics in performance_metrics.values()]\n",
    "        average_efficiency = sum(efficiency_scores) / len(efficiency_scores) if efficiency_scores else 0\n",
    "        \n",
    "        # Memory and resource analysis (mock implementation)\n",
    "        resource_analysis = {\n",
    "            \"peak_memory_usage\": \"~2.5GB\",  # Estimated\n",
    "            \"concurrent_operations\": 3,\n",
    "            \"api_calls_made\": sum(len(test.get(\"details\", {})) for test in test_suite.test_results[\"tests\"].values()),\n",
    "            \"model_inference_time\": sum(test[\"duration\"] for test in test_suite.test_results[\"tests\"].values() \n",
    "                                       if test.get(\"details\", {}).get(\"model_used\"))\n",
    "        }\n",
    "        \n",
    "        # Store performance results\n",
    "        test_suite.test_results[\"performance\"] = {\n",
    "            \"total_duration\": total_duration,\n",
    "            \"component_metrics\": performance_metrics,\n",
    "            \"average_efficiency\": average_efficiency,\n",
    "            \"resource_analysis\": resource_analysis,\n",
    "            \"bottlenecks\": [name for name, metrics in performance_metrics.items() \n",
    "                          if not metrics[\"within_threshold\"]]\n",
    "        }\n",
    "        \n",
    "        details = {\n",
    "            \"total_execution_time\": total_duration,\n",
    "            \"average_efficiency\": average_efficiency,\n",
    "            \"performance_bottlenecks\": len(test_suite.test_results[\"performance\"][\"bottlenecks\"]),\n",
    "            \"components_within_threshold\": sum(1 for metrics in performance_metrics.values() \n",
    "                                             if metrics[\"within_threshold\"])\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"completed\", details)\n",
    "        return test_suite.test_results[\"performance\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return {}\n",
    "\n",
    "# Run performance analysis\n",
    "performance_results = analyze_performance()\n",
    "print(f\"Performance Analysis Completed\")\n",
    "print(f\"Total Execution Time: {performance_results.get('total_duration', 0):.2f}s\")\n",
    "print(f\"Average Efficiency: {performance_results.get('average_efficiency', 0):.2f}\")\n",
    "print(f\"Performance Bottlenecks: {len(performance_results.get('bottlenecks', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_quality():\n",
    "    \"\"\"Assess overall quality of integration\"\"\"\n",
    "    test_name = \"quality_assessment\"\n",
    "    test_suite.log_test_start(test_name)\n",
    "    \n",
    "    try:\n",
    "        # Calculate test success rate\n",
    "        total_tests = len(test_suite.test_results[\"tests\"])\n",
    "        passed_tests = sum(1 for test in test_suite.test_results[\"tests\"].values() \n",
    "                         if test.get(\"status\") == \"passed\")\n",
    "        test_success_rate = passed_tests / total_tests if total_tests > 0 else 0\n",
    "        \n",
    "        # Error analysis\n",
    "        error_count = len(test_suite.test_results[\"errors\"])\n",
    "        error_rate = error_count / total_tests if total_tests > 0 else 0\n",
    "        \n",
    "        # Component integration assessment\n",
    "        integration_score = 0\n",
    "        if discovered_documents:\n",
    "            integration_score += 20  # Document discovery works\n",
    "        if extracted_tables:\n",
    "            integration_score += 20  # Table extraction works\n",
    "        if financial_analyses:\n",
    "            integration_score += 20  # Financial analysis works\n",
    "        if rag_results:\n",
    "            integration_score += 15  # RAG implementation works\n",
    "        if workflow_results:\n",
    "            integration_score += 15  # LangGraph workflow works\n",
    "        if crew_results:\n",
    "            integration_score += 10  # CrewAI agents work\n",
    "        \n",
    "        integration_score = integration_score / 100  # Normalize to 0-1\n",
    "        \n",
    "        # Data flow continuity\n",
    "        data_flow_continuity = {\n",
    "            \"documents_to_tables\": len(extracted_tables) > 0 if discovered_documents else False,\n",
    "            \"tables_to_analysis\": len(financial_analyses) > 0 if extracted_tables else False,\n",
    "            \"analysis_to_rag\": len(rag_results) > 0 if financial_analyses else False,\n",
    "            \"rag_to_workflow\": workflow_results is not None if rag_results else False,\n",
    "            \"workflow_to_agents\": crew_results is not None if workflow_results else False\n",
    "        }\n",
    "        \n",
    "        continuity_score = sum(data_flow_continuity.values()) / len(data_flow_continuity)\n",
    "        \n",
    "        # Overall quality score\n",
    "        quality_weights = {\n",
    "            \"test_success_rate\": 0.3,\n",
    "            \"integration_score\": 0.3,\n",
    "            \"continuity_score\": 0.25,\n",
    "            \"error_penalty\": 0.15\n",
    "        }\n",
    "        \n",
    "        overall_quality = (\n",
    "            test_success_rate * quality_weights[\"test_success_rate\"] +\n",
    "            integration_score * quality_weights[\"integration_score\"] +\n",
    "            continuity_score * quality_weights[\"continuity_score\"] +\n",
    "            (1 - error_rate) * quality_weights[\"error_penalty\"]\n",
    "        )\n",
    "        \n",
    "        # Store quality results\n",
    "        test_suite.test_results[\"quality_metrics\"] = {\n",
    "            \"test_success_rate\": test_success_rate,\n",
    "            \"error_rate\": error_rate,\n",
    "            \"integration_score\": integration_score,\n",
    "            \"continuity_score\": continuity_score,\n",
    "            \"data_flow_continuity\": data_flow_continuity,\n",
    "            \"overall_quality\": overall_quality,\n",
    "            \"quality_grade\": (\n",
    "                \"A\" if overall_quality >= 0.9 else\n",
    "                \"B\" if overall_quality >= 0.8 else\n",
    "                \"C\" if overall_quality >= 0.7 else\n",
    "                \"D\" if overall_quality >= 0.6 else \"F\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        details = {\n",
    "            \"overall_quality_score\": overall_quality,\n",
    "            \"quality_grade\": test_suite.test_results[\"quality_metrics\"][\"quality_grade\"],\n",
    "            \"test_success_rate\": test_success_rate,\n",
    "            \"integration_score\": integration_score,\n",
    "            \"data_continuity\": continuity_score\n",
    "        }\n",
    "        \n",
    "        test_suite.log_test_end(test_name, \"completed\", details)\n",
    "        return test_suite.test_results[\"quality_metrics\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        test_suite.log_error(test_name, e)\n",
    "        test_suite.log_test_end(test_name, \"failed\")\n",
    "        return {}\n",
    "\n",
    "# Run quality assessment\n",
    "quality_results = assess_quality()\n",
    "print(f\"Quality Assessment Completed\")\n",
    "print(f\"Overall Quality Score: {quality_results.get('overall_quality', 0):.2f}\")\n",
    "print(f\"Quality Grade: {quality_results.get('quality_grade', 'N/A')}\")\n",
    "print(f\"Test Success Rate: {quality_results.get('test_success_rate', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Test Summary and Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary():\n",
    "    \"\"\"Generate comprehensive test summary\"\"\"\n",
    "    # Calculate summary statistics\n",
    "    total_tests = len(test_suite.test_results[\"tests\"])\n",
    "    passed_tests = sum(1 for test in test_suite.test_results[\"tests\"].values() \n",
    "                      if test.get(\"status\") == \"passed\")\n",
    "    failed_tests = total_tests - passed_tests\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        \"test_execution\": {\n",
    "            \"total_tests\": total_tests,\n",
    "            \"passed_tests\": passed_tests,\n",
    "            \"failed_tests\": failed_tests,\n",
    "            \"success_rate\": passed_tests / total_tests if total_tests > 0 else 0\n",
    "        },\n",
    "        \"performance\": performance_results,\n",
    "        \"quality\": quality_results,\n",
    "        \"components_tested\": [\n",
    "            \"Document Discovery\",\n",
    "            \"Table Extraction (Multi-Model)\",\n",
    "            \"Financial Analysis (Claude 4)\",\n",
    "            \"RAG Implementation (HyDE + Contextual)\",\n",
    "            \"LangGraph Workflow\",\n",
    "            \"CrewAI Multi-Agent\"\n",
    "        ],\n",
    "        \"integration_status\": {\n",
    "            \"document_pipeline\": len(extracted_tables) > 0,\n",
    "            \"analysis_pipeline\": len(financial_analyses) > 0,\n",
    "            \"knowledge_pipeline\": len(rag_results) > 0,\n",
    "            \"workflow_orchestration\": workflow_results is not None,\n",
    "            \"agent_collaboration\": crew_results is not None\n",
    "        },\n",
    "        \"recommendations\": []\n",
    "    }\n",
    "    \n",
    "    # Generate recommendations based on results\n",
    "    if summary[\"test_execution\"][\"success_rate\"] < 0.8:\n",
    "        summary[\"recommendations\"].append(\"Address failing tests before production deployment\")\n",
    "    \n",
    "    if len(performance_results.get(\"bottlenecks\", [])) > 0:\n",
    "        summary[\"recommendations\"].append(f\"Optimize performance bottlenecks: {performance_results['bottlenecks']}\")\n",
    "    \n",
    "    if quality_results.get(\"overall_quality\", 0) < 0.8:\n",
    "        summary[\"recommendations\"].append(\"Improve integration quality before moving to production\")\n",
    "    \n",
    "    if not summary[\"integration_status\"][\"agent_collaboration\"]:\n",
    "        summary[\"recommendations\"].append(\"Fix CrewAI multi-agent integration issues\")\n",
    "    \n",
    "    # Store summary\n",
    "    test_suite.test_results[\"summary\"] = summary\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate final summary\n",
    "final_summary = generate_final_summary()\n",
    "\n",
    "# Save all results\n",
    "results_file = test_suite.save_results()\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" INTEGRATION TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total Tests: {final_summary['test_execution']['total_tests']}\")\n",
    "print(f\"Passed: {final_summary['test_execution']['passed_tests']}\")\n",
    "print(f\"Failed: {final_summary['test_execution']['failed_tests']}\")\n",
    "print(f\"Success Rate: {final_summary['test_execution']['success_rate']:.2%}\")\n",
    "print(f\"\")\n",
    "print(f\"Quality Grade: {quality_results.get('quality_grade', 'N/A')}\")\n",
    "print(f\"Overall Quality Score: {quality_results.get('overall_quality', 0):.2f}\")\n",
    "print(f\"Total Execution Time: {performance_results.get('total_duration', 0):.2f}s\")\n",
    "print(f\"\")\n",
    "print(\"Component Integration Status:\")\n",
    "for component, status in final_summary[\"integration_status\"].items():\n",
    "    status_emoji = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"  {status_emoji} {component.replace('_', ' ').title()}\")\n",
    "\n",
    "if final_summary[\"recommendations\"]:\n",
    "    print(f\"\\nRecommendations:\")\n",
    "    for i, rec in enumerate(final_summary[\"recommendations\"], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(f\"\\nDetailed results saved to: {results_file}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Resource Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary resources and close connections\n",
    "def cleanup_test_environment():\n",
    "    \"\"\"Clean up test environment and resources\"\"\"\n",
    "    try:\n",
    "        # Close any open database connections\n",
    "        # Close vector store connections\n",
    "        # Clear temporary files if any\n",
    "        # Reset model caches\n",
    "        \n",
    "        logger.info(\"Test environment cleanup completed\")\n",
    "        print(\"\\nðŸ§¹ Test environment cleaned up successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cleanup failed: {e}\")\n",
    "        print(f\"âš ï¸ Cleanup warning: {e}\")\n",
    "\n",
    "# Perform cleanup\n",
    "cleanup_test_environment()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Integration testing completed successfully!\")\n",
    "print(f\"ðŸ“Š Test results available at: {results_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}