{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from unstract.llmwhisperer import LLMWhispererClientV2\n",
    "# from unstract.llmwhisperer.client_v2 import LLMWhispererClientException\n",
    "\n",
    "# # Initialize client\n",
    "# client = LLMWhispererClientV2(api_key=api)\n",
    "\n",
    "# # Define output directory\n",
    "# output_dir = \"data/output/Quatersdata\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # First, verify file paths exist\n",
    "# pdf_list = [\n",
    "#     \"data/pdfs/Quarter I Ended FY 2025-26.pdf\",\n",
    "#     \"data/pdfs/Quarter III Ended FY 2024-25.pdf\", \n",
    "#     \"data/pdfs/Quarter IV & Year Ended FY 2024-25.pdf\"\n",
    "# ]\n",
    "\n",
    "# print(\"=== FILE VERIFICATION ===\")\n",
    "# valid_pdfs = []\n",
    "# for pdf_path in pdf_list:\n",
    "#     if os.path.exists(pdf_path):\n",
    "#         file_size = os.path.getsize(pdf_path) / (1024*1024)  # MB\n",
    "#         print(f\"✓ {pdf_path} exists ({file_size:.1f} MB)\")\n",
    "#         valid_pdfs.append(pdf_path)\n",
    "#     else:\n",
    "#         print(f\"✗ {pdf_path} NOT FOUND\")\n",
    "#         # Try to find it\n",
    "#         current_dir = os.getcwd()\n",
    "#         for root, dirs, files in os.walk(current_dir):\n",
    "#             if pdf_path.split('/')[-1] in files:\n",
    "#                 found_path = os.path.join(root, pdf_path.split('/')[-1])\n",
    "#                 print(f\"  Found at: {found_path}\")\n",
    "#                 valid_pdfs.append(found_path)\n",
    "#                 break\n",
    "\n",
    "# if not valid_pdfs:\n",
    "#     print(\"ERROR: No valid PDF files found. Please check paths.\")\n",
    "#     exit(1)\n",
    "\n",
    "# print(f\"\\nProcessing {len(valid_pdfs)} valid PDF(s)...\")\n",
    "\n",
    "# # Aggregate results\n",
    "# all_results = {\"documents\": [], \"processing_summary\": {\"successful\": 0, \"failed\": 0, \"total_pages\": 0}}\n",
    "\n",
    "# for pdf_path in valid_pdfs:\n",
    "#     doc_name = os.path.basename(pdf_path)\n",
    "#     doc_id = doc_name.replace('.pdf', '').replace(' ', '_').replace('&', 'and').replace('/', '_')\n",
    "    \n",
    "#     print(f\"\\n--- Processing {doc_name} ---\")\n",
    "    \n",
    "#     try:\n",
    "#         # Get file stats\n",
    "#         file_size_mb = os.path.getsize(pdf_path) / (1024*1024)\n",
    "#         print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "        \n",
    "#         # Check your usage first (if available)\n",
    "#         try:\n",
    "#             usage = client.get_usage_info() if hasattr(client, 'get_usage_info') else None\n",
    "#             if usage:\n",
    "#                 print(f\"API Usage: {usage}\")\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#         # Process with more detailed parameters\n",
    "#         result = client.whisper(\n",
    "#             file_path=pdf_path,\n",
    "#             wait_for_completion=True,\n",
    "#             wait_timeout=600,  # 10 minutes timeout\n",
    "#             output_mode=\"layout_preserving\",\n",
    "#             pages_to_extract=\"\",  # All pages\n",
    "#             mode=\"high_quality\",\n",
    "#             # Add these for better debugging\n",
    "#             page_seperator=\"<<<PAGE>>>\",\n",
    "#             add_line_nos=False,\n",
    "#             lang=\"eng\"\n",
    "#         )\n",
    "        \n",
    "#         print(f\"Raw result keys: {list(result.keys())}\")\n",
    "        \n",
    "#         # Handle different response structures\n",
    "#         extraction = None\n",
    "#         if 'extraction' in result:\n",
    "#             extraction = result['extraction']\n",
    "#         elif 'result' in result:\n",
    "#             extraction = result['result']\n",
    "#         elif isinstance(result, dict) and 'result_text' in result:\n",
    "#             extraction = result\n",
    "#         else:\n",
    "#             print(f\"Unexpected result structure: {type(result)} - {result}\")\n",
    "#             raise ValueError(f\"Unexpected response format: {result}\")\n",
    "        \n",
    "#         # Extract content\n",
    "#         extracted_text = extraction.get('result_text', '') or extraction.get('text', '') or ''\n",
    "#         metadata = extraction.get('metadata', {}) or {}\n",
    "        \n",
    "#         print(f\"Extracted text length: {len(extracted_text)} characters\")\n",
    "#         print(f\"Metadata keys: {list(metadata.keys())}\")\n",
    "        \n",
    "#         # Check if we actually got content\n",
    "#         if not extracted_text.strip():\n",
    "#             print(\"WARNING: No text extracted. This might be a scanned PDF or extraction failure.\")\n",
    "#             # Try OCR mode explicitly\n",
    "#             print(\"Retrying with OCR mode...\")\n",
    "#             result_ocr = client.whisper(\n",
    "#                 file_path=pdf_path,\n",
    "#                 wait_for_completion=True,\n",
    "#                 wait_timeout=600,\n",
    "#                 output_mode=\"layout_preserving\",\n",
    "#                 pages_to_extract=\"1-5\",  # Test first 5 pages\n",
    "#                 mode=\"ocr\"  # Force OCR\n",
    "#             )\n",
    "#             extraction_ocr = result_ocr.get('extraction', result_ocr)\n",
    "#             extracted_text = extraction_ocr.get('result_text', '')\n",
    "#             print(f\"OCR attempt - text length: {len(extracted_text)} characters\")\n",
    "        \n",
    "#         doc_result = {\n",
    "#             \"filename\": doc_name,\n",
    "#             \"document_id\": doc_id,\n",
    "#             \"file_path\": pdf_path,\n",
    "#             \"file_size_mb\": round(file_size_mb, 2),\n",
    "#             \"extracted_text\": extracted_text[:10000] if len(extracted_text) > 10000 else extracted_text,  # Truncate for storage\n",
    "#             \"full_text_length\": len(extracted_text),\n",
    "#             \"pages_processed\": metadata.get('pages_processed', 0),\n",
    "#             \"total_pages\": metadata.get('total_pages', 0),\n",
    "#             \"extraction_metadata\": metadata,\n",
    "#             \"success\": bool(extracted_text.strip())\n",
    "#         }\n",
    "        \n",
    "#         all_results['documents'].append(doc_result)\n",
    "        \n",
    "#         if doc_result['success']:\n",
    "#             all_results['processing_summary']['successful'] += 1\n",
    "#             all_results['processing_summary']['total_pages'] += doc_result['pages_processed']\n",
    "#             print(f\"✓ SUCCESS: {len(extracted_text)} chars, {doc_result['pages_processed']} pages\")\n",
    "#         else:\n",
    "#             all_results['processing_summary']['failed'] += 1\n",
    "#             print(f\"✗ FAILED: No content extracted\")\n",
    "        \n",
    "#         # Save individual document\n",
    "#         doc_output_file = os.path.join(output_dir, f\"{doc_id}_extraction.json\")\n",
    "#         with open(doc_output_file, 'w', encoding='utf-8') as f:\n",
    "#             # Save full text in a separate file if large\n",
    "#             if len(extracted_text) > 10000:\n",
    "#                 full_text_file = os.path.join(output_dir, f\"{doc_id}_full_text.txt\")\n",
    "#                 with open(full_text_file, 'w', encoding='utf-8') as ft:\n",
    "#                     ft.write(extracted_text)\n",
    "#                 doc_result['full_text_file'] = full_text_file\n",
    "#                 doc_result['extracted_text'] = extracted_text[:1000] + \"\\n...[truncated]...\\n\"\n",
    "            \n",
    "#             json.dump(doc_result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "#         # Preview first 500 chars\n",
    "#         if extracted_text:\n",
    "#             print(f\"Preview:\\n{extracted_text[:500]}...\\n\")\n",
    "        \n",
    "#         # Simple table detection in preview\n",
    "#         if '|' in extracted_text[:2000] or '-' * 10 in extracted_text[:2000]:\n",
    "#             print(\"Detected potential table content!\")\n",
    "            \n",
    "#             # Save full text for table analysis\n",
    "#             full_text_file = os.path.join(output_dir, f\"{doc_id}_full_text_for_tables.txt\")\n",
    "#             with open(full_text_file, 'w', encoding='utf-8') as f:\n",
    "#                 f.write(extracted_text)\n",
    "#             print(f\"Full text saved for table analysis: {full_text_file}\")\n",
    "        \n",
    "#         time.sleep(2)  # Rate limiting\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR processing {doc_name}: {type(e).__name__}: {str(e)}\")\n",
    "#         all_results['documents'].append({\n",
    "#             \"filename\": doc_name, \n",
    "#             \"error\": f\"{type(e).__name__}: {str(e)}\",\n",
    "#             \"success\": False\n",
    "#         })\n",
    "#         all_results['processing_summary']['failed'] += 1\n",
    "\n",
    "# # Save complete results\n",
    "# complete_file = os.path.join(output_dir, \"quarterly_financial_data.json\")\n",
    "# with open(complete_file, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"\\n=== SUMMARY ===\")\n",
    "# print(f\"Processed: {all_results['processing_summary']['successful']} successful, {all_results['processing_summary']['failed']} failed\")\n",
    "# print(f\"Total pages extracted: {all_results['processing_summary']['total_pages']}\")\n",
    "# print(f\"All results saved to: {complete_file}\")\n",
    "\n",
    "# # Show what's in the output directory\n",
    "# print(f\"\\nFiles in {output_dir}:\")\n",
    "# for file in os.listdir(output_dir):\n",
    "#     file_path = os.path.join(output_dir, file)\n",
    "#     size = os.path.getsize(file_path) / 1024  # KB\n",
    "#     print(f\"  {file} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (4.56.1)\n",
      "Requirement already satisfied: torch in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: pillow in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (11.3.0)\n",
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pdf2image\n",
      "Successfully installed pdf2image-1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch pillow pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from timm) (2.8.0)\n",
      "Requirement already satisfied: torchvision in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from timm) (0.23.0)\n",
      "Requirement already satisfied: pyyaml in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from timm) (0.35.0)\n",
      "Requirement already satisfied: safetensors in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: filelock in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: requests in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: numpy in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torchvision->timm) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/maple/Downloads/financialagent/venv/lib/python3.10/site-packages (from torchvision->timm) (11.3.0)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading timm-1.0.19-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops, timm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [timm][32m1/2\u001b[0m [timm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 timm-1.0.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install einops timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Florence-2 compatibility...\n",
      "🚀 Loading Microsoft Florence-2...\n",
      "  🔄 Attempting to load: microsoft/Florence-2-large-ft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eac377080b48b58d80861cbbd82cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a0118e334d42cfa200c092c24045cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Failed to load microsoft/Florence-2-large-ft: 'Florence2ForConditionalGeneration' object has no attribute '_supports_sdpa'\n",
      "  🔄 Trying fallback model...\n",
      "  🔄 Loading fallback: microsoft/Florence-2-base-ft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b73b1d40514d13ac604ea8f9c81eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1474311f5254b83ad5445ada635b543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Loaded fallback: microsoft/Florence-2-base-ft\n",
      "✅ Florence-2 loaded successfully!\n",
      "📱 Device: CPU\n",
      "💾 Model dtype: torch.float32\n",
      "\n",
      "🔍 Testing extraction from: data/pdfs/Quarter I Ended FY 2025-26.pdf\n",
      "\n",
      "📄 Processing page 2 from Quarter I Ended FY 2025-26\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running performance_highlights task...\n",
      "  🔍 Running task: detailed_caption\n",
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ performance_highlights completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "✅ Page 2 extraction successful!\n",
      "\n",
      "📊 Extracted metrics (0):\n",
      "\n",
      "📝 OCR Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "🚀 Processing 3 valid PDFs...\n",
      "\n",
      "🔄 Processing 3 TCS quarterly reports with Florence-2\n",
      "💻 Using device: CPU\n",
      "\n",
      "============================================================\n",
      "📁 Processing: Quarter I Ended FY 2025-26\n",
      "============================================================\n",
      "\n",
      "📄 Processing page 2 from Quarter I Ended FY 2025-26\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running performance_highlights task...\n",
      "  🔍 Running task: detailed_caption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ performance_highlights completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "\n",
      "📄 Processing page 3 from Quarter I Ended FY 2025-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running growth_summary task...\n",
      "  🔍 Running task: detailed_caption\n",
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ growth_summary completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "\n",
      "📄 Processing page 4 from Quarter I Ended FY 2025-26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running document_analysis task...\n",
      "  🔍 Running task: detailed_caption\n",
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    📈 Extracted table with 1 rows\n",
      "    ✅ document_analysis completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "  ⚠ No metrics extracted from Quarter I Ended FY 2025-26\n",
      "\n",
      "============================================================\n",
      "📁 Processing: Quarter III Ended FY 2024-25\n",
      "============================================================\n",
      "\n",
      "📄 Processing page 2 from Quarter III Ended FY 2024-25\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running performance_highlights task...\n",
      "  🔍 Running task: detailed_caption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ performance_highlights completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "\n",
      "📄 Processing page 3 from Quarter III Ended FY 2024-25\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running growth_summary task...\n",
      "  🔍 Running task: detailed_caption\n",
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ growth_summary completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "\n",
      "📄 Processing page 4 from Quarter III Ended FY 2024-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running document_analysis task...\n",
      "  🔍 Running task: detailed_caption\n",
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    📈 Extracted table with 1 rows\n",
      "    ✅ document_analysis completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "  ⚠ No metrics extracted from Quarter III Ended FY 2024-25\n",
      "\n",
      "============================================================\n",
      "📁 Processing: Quarter IV & Year Ended FY 2024-25\n",
      "============================================================\n",
      "\n",
      "📄 Processing page 2 from Quarter IV & Year Ended FY 2024-25\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running performance_highlights task...\n",
      "  🔍 Running task: detailed_caption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ performance_highlights completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "\n",
      "📄 Processing page 3 from Quarter IV & Year Ended FY 2024-25\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running growth_summary task...\n",
      "  🔍 Running task: detailed_caption\n",
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    ✅ growth_summary completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "\n",
      "📄 Processing page 4 from Quarter IV & Year Ended FY 2024-25\n",
      "  🖼️  Image loaded: (2000, 1125)\n",
      "  🔄 Running document_analysis task...\n",
      "  🔍 Running task: detailed_caption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ Generation error: Task token <MORE_DETAILED_CAPTION> should be the only token in the text.\n",
      "    📈 Extracted table with 1 rows\n",
      "    ✅ document_analysis completed (97 chars)\n",
      "    📝 Preview: Error during extraction: Task token <MORE_DETAILED_CAPTION> should be the only token in the text....\n",
      "\n",
      "  🔄 Running full_ocr task...\n",
      "  🔍 Running task: ocr\n",
      "  ❌ Generation error: 'NoneType' object has no attribute 'shape'\n",
      "    ✅ full_ocr completed (67 chars)\n",
      "    📝 Preview: Error during extraction: 'NoneType' object has no attribute 'shape'...\n",
      "\n",
      "  ⚠ No metrics extracted from Quarter IV & Year Ended FY 2024-25\n",
      "\n",
      "📊 Comparison table created:\n",
      "                          Document  Revenue_INR_Mn  Revenue_USD_Mn  YoY_Growth_%  CC_Growth_%  Operating_Margin_%  Net_Margin_%  Headcount  Attrition_%  Order_Book_Bn\n",
      "        Quarter I Ended FY 2025-26               0               0             0            0                   0             0          0            0              0\n",
      "      Quarter III Ended FY 2024-25               0               0             0            0                   0             0          0            0              0\n",
      "Quarter IV & Year Ended FY 2024-25               0               0             0            0                   0             0          0            0              0\n",
      "📁 CSV saved: data/output/florence2_results/tcs_quarterly_comparison.csv\n",
      "\n",
      "🎉 Florence-2 extraction complete!\n",
      "📁 Results saved to: data/output/florence2_results\n",
      "📊 Total documents processed: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Florence2FinancialExtractor:\n",
    "    def __init__(self, model_name=\"microsoft/Florence-2-large-ft\"):\n",
    "        \"\"\"\n",
    "        Fixed Florence-2 extractor with proper fallback handling\n",
    "        \"\"\"\n",
    "        print(\"🚀 Loading Microsoft Florence-2...\")\n",
    "        \n",
    "        # Initialize task tokens FIRST (before any model loading)\n",
    "        self.task_tokens = {\n",
    "            \"ocr\": \"<OCR_WITH_REGION>\",\n",
    "            \"caption\": \"<CAPTION>\",\n",
    "            \"detailed_caption\": \"<MORE_DETAILED_CAPTION>\",\n",
    "            \"region_caption\": \"<MORE_DETAILED_CAPTION>\",\n",
    "            \"region_detection\": \"<OD>\"\n",
    "        }\n",
    "        \n",
    "        # Try to load the preferred model\n",
    "        success = False\n",
    "        try:\n",
    "            print(f\"  🔄 Attempting to load: {model_name}\")\n",
    "            self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                attn_implementation=\"eager\" if torch.cuda.is_available() else None,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            success = True\n",
    "            print(f\"  ✅ Loaded {model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed to load {model_name}: {e}\")\n",
    "            print(\"  🔄 Trying fallback model...\")\n",
    "        \n",
    "        # Fallback to base model if large model fails\n",
    "        if not success:\n",
    "            try:\n",
    "                fallback_model = \"microsoft/Florence-2-base-ft\"\n",
    "                print(f\"  🔄 Loading fallback: {fallback_model}\")\n",
    "                self.processor = AutoProcessor.from_pretrained(fallback_model, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    fallback_model,\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    attn_implementation=\"eager\"\n",
    "                )\n",
    "                success = True\n",
    "                print(f\"  ✅ Loaded fallback: {fallback_model}\")\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"  ❌ Fallback failed: {e2}\")\n",
    "                print(\"  💥 All model loading attempts failed. Check your transformers version.\")\n",
    "                print(\"  🔧 Try: pip install transformers==4.36.0\")\n",
    "                raise e2\n",
    "        \n",
    "        # Final success check\n",
    "        if success:\n",
    "            print(\"✅ Florence-2 loaded successfully!\")\n",
    "            print(f\"📱 Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "            print(f\"💾 Model dtype: {next(self.model.parameters()).dtype}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Failed to load any Florence-2 model\")\n",
    "    \n",
    "    def extract_with_florence2(self, image: Image.Image, task: str, text_prompt: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Florence-2 extraction method with proper error handling\n",
    "        \"\"\"\n",
    "        print(f\"  🔍 Running task: {task}\")\n",
    "        \n",
    "        # Use the task token (now guaranteed to exist)\n",
    "        task_token = self.task_tokens.get(task, \"<MORE_DETAILED_CAPTION>\")\n",
    "        \n",
    "        # Prepare text input\n",
    "        if text_prompt:\n",
    "            full_text = f\"{task_token}\\n{text_prompt}\"\n",
    "        else:\n",
    "            full_text = task_token\n",
    "        \n",
    "        try:\n",
    "            # Process image and text\n",
    "            inputs = self.processor(\n",
    "                text=full_text, \n",
    "                images=image, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            device = next(self.model.parameters()).device if hasattr(self.model, 'parameters') else torch.device(\"cpu\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    pixel_values=inputs[\"pixel_values\"], \n",
    "                    max_new_tokens=512,\n",
    "                    num_beams=1,  # Single beam for faster processing\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                    output_hidden_states=False,\n",
    "                    output_attentions=False\n",
    "                )\n",
    "            \n",
    "            # Decode response\n",
    "            generated_text = self.processor.batch_decode(\n",
    "                generated_ids.sequences, skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            # Clean up response\n",
    "            if task_token == \"<OCR_WITH_REGION>\":\n",
    "                generated_text = re.sub(r'<.*?>', '', generated_text)\n",
    "            \n",
    "            return generated_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Generation error: {e}\")\n",
    "            return f\"Error during extraction: {str(e)}\"\n",
    "    \n",
    "    def process_page_with_florence(self, pdf_path: str, page_num: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process specific page with Florence-2\n",
    "        \"\"\"\n",
    "        print(f\"\\n📄 Processing page {page_num} from {Path(pdf_path).stem}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert PDF page to image\n",
    "            images = convert_from_path(pdf_path, first_page=page_num, last_page=page_num, dpi=150)\n",
    "            if not images:\n",
    "                raise ValueError(f\"No image generated for page {page_num}\")\n",
    "            \n",
    "            image = images[0]\n",
    "            print(f\"  🖼️  Image loaded: {image.size}\")\n",
    "            \n",
    "            # Define tasks for this page\n",
    "            if page_num == 2:  # Performance highlights page\n",
    "                tasks = {\n",
    "                    \"performance_highlights\": {\n",
    "                        \"task\": \"detailed_caption\",\n",
    "                        \"prompt\": \"Extract all financial metrics from this TCS quarterly performance highlights page. Look for Revenue (INR/USD), Growth rates (YoY, CC), Margins (Operating, Net), Cash flow, Headcount, Attrition, Order book TCV, Client metrics.\"\n",
    "                    },\n",
    "                    \"full_ocr\": {\n",
    "                        \"task\": \"ocr\",\n",
    "                        \"prompt\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            elif page_num == 3:  # Growth summary page\n",
    "                tasks = {\n",
    "                    \"growth_summary\": {\n",
    "                        \"task\": \"detailed_caption\",\n",
    "                        \"prompt\": \"This is a financial growth summary chart. Extract quarterly data: quarters, revenue values, operating income, net income, growth percentages, margins.\"\n",
    "                    },\n",
    "                    \"full_ocr\": {\n",
    "                        \"task\": \"ocr\",\n",
    "                        \"prompt\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            else:  # Other pages\n",
    "                tasks = {\n",
    "                    \"document_analysis\": {\n",
    "                        \"task\": \"detailed_caption\",\n",
    "                        \"prompt\": \"Analyze this financial document page and extract any tables, charts, or key metrics.\"\n",
    "                    },\n",
    "                    \"full_ocr\": {\n",
    "                        \"task\": \"ocr\",\n",
    "                        \"prompt\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            for task_name, config in tasks.items():\n",
    "                print(f\"  🔄 Running {task_name} task...\")\n",
    "                \n",
    "                result = self.extract_with_florence2(\n",
    "                    image=image,\n",
    "                    task=config[\"task\"],\n",
    "                    text_prompt=config[\"prompt\"]\n",
    "                )\n",
    "                \n",
    "                results[task_name] = {\n",
    "                    \"raw_response\": result,\n",
    "                    \"task\": config[\"task\"],\n",
    "                    \"prompt\": config[\"prompt\"]\n",
    "                }\n",
    "                \n",
    "                # Parse results based on task type\n",
    "                if \"highlights\" in task_name or \"growth\" in task_name:\n",
    "                    parsed_metrics = self._parse_performance_metrics(result)\n",
    "                    results[task_name][\"parsed_metrics\"] = parsed_metrics\n",
    "                    if parsed_metrics:\n",
    "                        print(f\"    📊 Extracted {len(parsed_metrics)} metrics\")\n",
    "                \n",
    "                elif \"document\" in task_name:\n",
    "                    parsed_table = self._parse_table_data(result)\n",
    "                    results[task_name][\"parsed_table\"] = parsed_table\n",
    "                    if parsed_table:\n",
    "                        print(f\"    📈 Extracted table with {len(parsed_table)} rows\")\n",
    "                \n",
    "                # Always do OCR parsing\n",
    "                if \"full_ocr\" in task_name:\n",
    "                    ocr_metrics = self._parse_ocr_metrics(result)\n",
    "                    results[task_name][\"parsed_ocr\"] = ocr_metrics\n",
    "                    if ocr_metrics:\n",
    "                        print(f\"    📝 OCR extracted {len(ocr_metrics)} items\")\n",
    "                \n",
    "                print(f\"    ✅ {task_name} completed ({len(result)} chars)\")\n",
    "                print(f\"    📝 Preview: {result[:150]}...\")\n",
    "                print()\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"page_number\": page_num,\n",
    "                \"image_size\": image.size,\n",
    "                \"results\": results\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error processing page {page_num}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"page_number\": page_num,\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "    \n",
    "    def _parse_performance_metrics(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse financial metrics from Florence-2 response\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # More robust patterns for financial data\n",
    "        patterns = {\n",
    "            \"revenue_inr\": [\n",
    "                r\"INR Revenue[^{}]*?₹?([,\\d]+)\\s*(?:Mn|million)\", \n",
    "                r\"₹([,\\d]+)\\s*Mn\", \n",
    "                r\"Revenue[^{}]*?₹([,\\d]+)\"\n",
    "            ],\n",
    "            \"revenue_usd\": [\n",
    "                r\"USD Revenue[^{}]*?\\$([,\\d]+)\\s*(?:Mn|million)\", \n",
    "                r\"\\$([,\\d]+)\\s*Mn\", \n",
    "                r\"Revenue[^{}]*?\\$([,\\d]+)\"\n",
    "            ],\n",
    "            \"yoy_growth\": [\n",
    "                r\"(?:up|YoY)[^{}]*?([\\d.-]+)%\", \n",
    "                r\"([\\d.-]+)%\\s*YoY\",\n",
    "                r\"Growth[^{}]*?([\\d.-]+)%\"\n",
    "            ],\n",
    "            \"cc_growth\": [\n",
    "                r\"Constant currency[^{}]*?(?:up|down)[^{}]*?([\\d.-]+)%\", \n",
    "                r\"CC[^{}]*?([\\d.-]+)%\",\n",
    "                r\"Constant[^{}]*?([\\d.-]+)%\"\n",
    "            ],\n",
    "            \"operating_margin\": [\n",
    "                r\"Operating Margin[^{}]*?([\\d.-]+)%\", \n",
    "                r\"Operating[^{}]*?([\\d.-]+)%\",\n",
    "                r\"Margin[^{}]*?([\\d.-]+)%\\s*Operating\"\n",
    "            ],\n",
    "            \"net_margin\": [\n",
    "                r\"Net Margin[^{}]*?([\\d.-]+)%\", \n",
    "                r\"Net[^{}]*?([\\d.-]+)%\",\n",
    "                r\"Margin[^{}]*?([\\d.-]+)%\\s*Net\"\n",
    "            ],\n",
    "            \"cash_flow\": [\n",
    "                r\"Cash flow[^{}]*?([\\d.-]+)%\", \n",
    "                r\"Cash[^{}]*?([\\d.-]+)%\\s*of\",\n",
    "                r\"([\\d.-]+)%\\s*of net profit\"\n",
    "            ],\n",
    "            \"headcount\": [\n",
    "                r\"headcount[^{}]*?:?([,\\d]+)\", \n",
    "                r\"([,\\d]+)\\s*employees?\",\n",
    "                r\"Total[^{}]*?([,\\d]+)\"\n",
    "            ],\n",
    "            \"attrition\": [\n",
    "                r\"attrition[^{}]*?([\\d.-]+)%\", \n",
    "                r\"([\\d.-]+)%\\s*attrition\",\n",
    "                r\"([\\d.-]+)%\\s*LTM\"\n",
    "            ],\n",
    "            \"order_book\": [\n",
    "                r\"Order book TCV[^{}]*?\\$([,\\d.-]+)\\s*Bn\", \n",
    "                r\"TCV[^{}]*?\\$([,\\d.-]+)\\s*Bn\",\n",
    "                r\"Order[^{}]*?\\$([,\\d.-]+)\\s*Bn\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for key, pattern_list in patterns.items():\n",
    "            for pattern in pattern_list:\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "                if matches:\n",
    "                    # Take the first match and clean it\n",
    "                    value = re.sub(r'[,\\s]', '', matches[0])\n",
    "                    try:\n",
    "                        # Handle negative numbers\n",
    "                        if value.startswith('-'):\n",
    "                            metrics[key] = float(value)\n",
    "                        else:\n",
    "                            metrics[key] = float(value)\n",
    "                    except ValueError:\n",
    "                        metrics[key] = value\n",
    "                    break  # Found a match, stop searching\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _parse_ocr_metrics(self, ocr_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse metrics directly from OCR output\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Direct pattern matching from OCR text\n",
    "        ocr_patterns = {\n",
    "            \"revenue_inr_raw\": r\"₹\\s*([\\d,]+)\\s*Mn\",\n",
    "            \"revenue_usd_raw\": r\"\\$\\s*([\\d,]+)\\s*Mn\",\n",
    "            \"growth_pct\": r\"([\\d.-]+)%\",\n",
    "            \"margin_pct\": r\"Margin\\s*[\\s\\w]*\\s*([\\d.-]+)%\",\n",
    "            \"headcount_raw\": r\"headcount\\s*:\\s*([\\d,]+)\",\n",
    "            \"attrition_raw\": r\"attrition\\s*([\\d.-]+)%\"\n",
    "        }\n",
    "        \n",
    "        for key, pattern in ocr_patterns.items():\n",
    "            matches = re.findall(pattern, ocr_text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                value = re.sub(r'[,\\s]', '', matches[0])\n",
    "                try:\n",
    "                    metrics[key] = float(value)\n",
    "                except ValueError:\n",
    "                    metrics[key] = value\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _parse_table_data(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Parse table data from Florence-2 response\"\"\"\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        \n",
    "        # Look for table-like structures\n",
    "        table_rows = []\n",
    "        current_row = {}\n",
    "        \n",
    "        for line in lines:\n",
    "            # Look for key-value pairs\n",
    "            if ':' in line and len(line.split(':')) == 2:\n",
    "                key, value = line.split(':', 1)\n",
    "                current_row[key.strip()] = value.strip()\n",
    "            \n",
    "            # Look for quarter patterns\n",
    "            quarter_match = re.search(r'(Q[1-4]\\s*FY\\d{2}-\\d{2})', line)\n",
    "            if quarter_match:\n",
    "                current_row['quarter'] = quarter_match.group(1)\n",
    "            \n",
    "            # Look for numeric values with labels\n",
    "            if re.search(r'[\\$₹]\\s*[\\d,]+\\.?\\d*\\s*(Mn|Bn)', line):\n",
    "                # Extract labeled numbers\n",
    "                num_match = re.search(r'([A-Za-z\\s]+?)\\s*[\\$₹]\\s*([\\d,]+)\\.?\\d*\\s*(Mn|Bn)', line)\n",
    "                if num_match:\n",
    "                    label = num_match.group(1).strip()\n",
    "                    value = num_match.group(2).replace(',', '')\n",
    "                    unit = num_match.group(3)\n",
    "                    current_row[f\"{label}_{unit}\"] = float(value)\n",
    "            \n",
    "            # If we have a complete row, add it\n",
    "            if len(current_row) >= 2:\n",
    "                table_rows.append(current_row.copy())\n",
    "                current_row = {}\n",
    "        \n",
    "        return table_rows if table_rows else [{\"raw_text\": text}]\n",
    "    \n",
    "    def process_all_tcs_reports(self, pdf_list: List[str], output_dir: str = \"data/output/florence2_results\"):\n",
    "        \"\"\"\n",
    "        Process all TCS quarterly reports with Florence-2\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        all_results = []\n",
    "        print(f\"\\n🔄 Processing {len(pdf_list)} TCS quarterly reports with Florence-2\")\n",
    "        print(f\"💻 Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        \n",
    "        for pdf_path in pdf_list:\n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"❌ File not found: {pdf_path}\")\n",
    "                continue\n",
    "            \n",
    "            doc_name = Path(pdf_path).stem\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"📁 Processing: {doc_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Process key pages\n",
    "            pages_to_process = [2, 3, 4]  # Page 2: Highlights, Page 3: Growth, Page 4: Additional\n",
    "            doc_results = {\n",
    "                \"document\": doc_name,\n",
    "                \"pages_processed\": [],\n",
    "                \"summary_metrics\": {},\n",
    "                \"extraction_date\": pd.Timestamp.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            for page_num in pages_to_process:\n",
    "                try:\n",
    "                    page_result = self.process_page_with_florence(pdf_path, page_num)\n",
    "                    \n",
    "                    if page_result[\"success\"]:\n",
    "                        doc_results[\"pages_processed\"].append(page_result)\n",
    "                        \n",
    "                        # Aggregate metrics\n",
    "                        for task_name, task_result in page_result[\"results\"].items():\n",
    "                            if \"parsed_metrics\" in task_result and task_result[\"parsed_metrics\"]:\n",
    "                                doc_results[\"summary_metrics\"].update(task_result[\"parsed_metrics\"])\n",
    "                    \n",
    "                    # Save page result\n",
    "                    page_file = os.path.join(output_dir, f\"{doc_name}_page_{page_num}.json\")\n",
    "                    with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(page_result, f, indent=2, default=str, ensure_ascii=False)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing page {page_num}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save document results\n",
    "            doc_file = os.path.join(output_dir, f\"{doc_name}_complete.json\")\n",
    "            with open(doc_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(doc_results, f, indent=2, default=str, ensure_ascii=False)\n",
    "            \n",
    "            all_results.append(doc_results)\n",
    "            \n",
    "            # Print summary\n",
    "            metrics = doc_results[\"summary_metrics\"]\n",
    "            if metrics:\n",
    "                print(f\"  📊 Key metrics extracted ({len(metrics)}):\")\n",
    "                for key, value in list(metrics.items())[:5]:  # Show first 5\n",
    "                    print(f\"    {key}: {value}\")\n",
    "                if len(metrics) > 5:\n",
    "                    print(f\"    ... and {len(metrics)-5} more\")\n",
    "            else:\n",
    "                print(f\"  ⚠ No metrics extracted from {doc_name}\")\n",
    "        \n",
    "        # Create comparison table\n",
    "        self._create_comparison_table(all_results, output_dir)\n",
    "        \n",
    "        # Save comprehensive summary\n",
    "        summary_file = os.path.join(output_dir, \"tcs_florence2_summary.json\")\n",
    "        with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_results, f, indent=2, default=str, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n🎉 Florence-2 extraction complete!\")\n",
    "        print(f\"📁 Results saved to: {output_dir}\")\n",
    "        print(f\"📊 Total documents processed: {len([r for r in all_results if 'summary_metrics' in r])}\")\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _create_comparison_table(self, results: List[Dict], output_dir: str):\n",
    "        \"\"\"Create cross-quarter comparison table\"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for result in results:\n",
    "            metrics = result.get(\"summary_metrics\", {})\n",
    "            row = {\n",
    "                \"Document\": result[\"document\"],\n",
    "                \"Revenue_INR_Mn\": metrics.get(\"revenue_inr\", 0),\n",
    "                \"Revenue_USD_Mn\": metrics.get(\"revenue_usd\", 0),\n",
    "                \"YoY_Growth_%\": metrics.get(\"yoy_growth\", 0),\n",
    "                \"CC_Growth_%\": metrics.get(\"cc_growth\", 0),\n",
    "                \"Operating_Margin_%\": metrics.get(\"operating_margin\", 0),\n",
    "                \"Net_Margin_%\": metrics.get(\"net_margin\", 0),\n",
    "                \"Headcount\": metrics.get(\"headcount\", 0),\n",
    "                \"Attrition_%\": metrics.get(\"attrition\", 0),\n",
    "                \"Order_Book_Bn\": metrics.get(\"order_book\", 0)\n",
    "            }\n",
    "            comparison_data.append(row)\n",
    "        \n",
    "        if comparison_data:\n",
    "            df = pd.DataFrame(comparison_data)\n",
    "            \n",
    "            # Clean and format numbers\n",
    "            numeric_cols = [\"Revenue_INR_Mn\", \"Revenue_USD_Mn\", \"Headcount\", \"Order_Book_Bn\"]\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            percent_cols = [\"YoY_Growth_%\", \"CC_Growth_%\", \"Operating_Margin_%\", \"Net_Margin_%\", \"Attrition_%\"]\n",
    "            for col in percent_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Save CSV\n",
    "            csv_file = os.path.join(output_dir, \"tcs_quarterly_comparison.csv\")\n",
    "            df.to_csv(csv_file, index=False)\n",
    "            \n",
    "            print(f\"\\n📊 Comparison table created:\")\n",
    "            print(df.round(2).to_string(index=False))\n",
    "            print(f\"📁 CSV saved: {csv_file}\")\n",
    "\n",
    "# Fixed Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test the completely fixed extractor\n",
    "    try:\n",
    "        print(\"🧪 Testing Florence-2 compatibility...\")\n",
    "        florence_extractor = Florence2FinancialExtractor()\n",
    "        \n",
    "        # Test single page extraction\n",
    "        sample_pdf = \"data/pdfs/Quarter I Ended FY 2025-26.pdf\"\n",
    "        if os.path.exists(sample_pdf):\n",
    "            print(f\"\\n🔍 Testing extraction from: {sample_pdf}\")\n",
    "            sample_results = florence_extractor.process_page_with_florence(sample_pdf, page_num=2)\n",
    "            \n",
    "            if sample_results[\"success\"]:\n",
    "                print(f\"✅ Page 2 extraction successful!\")\n",
    "                results = sample_results[\"results\"]\n",
    "                \n",
    "                # Print performance highlights\n",
    "                if \"performance_highlights\" in results:\n",
    "                    metrics = results[\"performance_highlights\"].get(\"parsed_metrics\", {})\n",
    "                    print(f\"\\n📊 Extracted metrics ({len(metrics)}):\")\n",
    "                    for key, value in metrics.items():\n",
    "                        print(f\"  {key}: {value}\")\n",
    "                \n",
    "                # Print OCR preview\n",
    "                if \"full_ocr\" in results:\n",
    "                    ocr_text = results[\"full_ocr\"][\"raw_response\"][:300]\n",
    "                    print(f\"\\n📝 OCR Preview: {ocr_text}...\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"❌ Extraction failed: {sample_results.get('error', 'Unknown error')}\")\n",
    "                if \"traceback\" in sample_results:\n",
    "                    print(\"Traceback:\")\n",
    "                    print(sample_results[\"traceback\"][:500] + \"...\")\n",
    "        else:\n",
    "            print(f\"❌ Test PDF not found: {sample_pdf}\")\n",
    "            print(\"Please ensure your PDFs are in data/pdfs/ directory\")\n",
    "        \n",
    "        # Process all documents\n",
    "        pdf_list = [\n",
    "            \"data/pdfs/Quarter I Ended FY 2025-26.pdf\",\n",
    "            \"data/pdfs/Quarter III Ended FY 2024-25.pdf\",\n",
    "            \"data/pdfs/Quarter IV & Year Ended FY 2024-25.pdf\"\n",
    "        ]\n",
    "        \n",
    "        valid_pdfs = [p for p in pdf_list if os.path.exists(p)]\n",
    "        if valid_pdfs:\n",
    "            print(f\"\\n🚀 Processing {len(valid_pdfs)} valid PDFs...\")\n",
    "            results = florence_extractor.process_all_tcs_reports(valid_pdfs)\n",
    "        else:\n",
    "            print(\"❌ No valid PDF files found!\")\n",
    "            print(\"Available files in data/pdfs/:\")\n",
    "            if os.path.exists(\"data/pdfs\"):\n",
    "                for file in os.listdir(\"data/pdfs\"):\n",
    "                    print(f\"  - {file}\")\n",
    "            else:\n",
    "                print(\"  - Directory 'data/pdfs/' doesn't exist\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
