{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Discovery & Web Scraping Experiment\n",
    "\n",
    "This notebook tests web scraping capabilities for TCS financial documents from screener.in and implements document classification logic.\n",
    "\n",
    "## Objectives:\n",
    "1. Test web scraping from screener.in/company/TCS/consolidated/#documents\n",
    "2. Implement document classification (quarterly reports, earnings calls, etc.)\n",
    "3. Validate document download and storage mechanisms\n",
    "4. Test document metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://www.screener.in/company/TCS/consolidated/#documents\"\n",
    "DOWNLOAD_DIR = \"data/raw_documents\"\n",
    "MAX_RETRIES = 3\n",
    "DELAY_BETWEEN_REQUESTS = 1  # seconds\n",
    "\n",
    "# Create download directory\n",
    "os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "# Headers to mimic browser request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "print(f\"Download directory: {DOWNLOAD_DIR}\")\n",
    "print(f\"Target URL: {BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page_content(url, retries=MAX_RETRIES):\n",
    "    \"\"\"\n",
    "    Fetch page content with retry logic\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS * (attempt + 1))\n",
    "            else:\n",
    "                raise\n",
    "    return None\n",
    "\n",
    "# Test page fetching\n",
    "try:\n",
    "    page_content = fetch_page_content(BASE_URL)\n",
    "    print(f\"✅ Successfully fetched page content ({len(page_content)} characters)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to fetch page: {e}\")\n",
    "    page_content = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_document_links(html_content):\n",
    "    \"\"\"\n",
    "    Extract document links from screener.in page\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    documents = []\n",
    "    \n",
    "    # Look for document links (adapt selectors based on actual page structure)\n",
    "    # This is a placeholder - actual selectors need to be determined by inspecting the page\n",
    "    \n",
    "    # Common patterns for financial document links\n",
    "    link_patterns = [\n",
    "        'a[href*=\"annual\"]',\n",
    "        'a[href*=\"quarterly\"]', \n",
    "        'a[href*=\".pdf\"]',\n",
    "        'a[href*=\"result\"]',\n",
    "        'a[href*=\"earnings\"]'\n",
    "    ]\n",
    "    \n",
    "    for pattern in link_patterns:\n",
    "        links = soup.select(pattern)\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            text = link.get_text(strip=True)\n",
    "            \n",
    "            if href and text:\n",
    "                documents.append({\n",
    "                    'title': text,\n",
    "                    'url': urljoin(BASE_URL, href),\n",
    "                    'type': classify_document_type(text),\n",
    "                    'discovered_at': datetime.now().isoformat()\n",
    "                })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def classify_document_type(title):\n",
    "    \"\"\"\n",
    "    Classify document based on title\n",
    "    \"\"\"\n",
    "    title_lower = title.lower()\n",
    "    \n",
    "    if any(term in title_lower for term in ['annual', 'yearly', '10-k']):\n",
    "        return 'annual_report'\n",
    "    elif any(term in title_lower for term in ['quarterly', 'q1', 'q2', 'q3', 'q4', '10-q']):\n",
    "        return 'quarterly_report'\n",
    "    elif any(term in title_lower for term in ['earnings', 'call', 'transcript']):\n",
    "        return 'earnings_call'\n",
    "    elif any(term in title_lower for term in ['presentation', 'investor']):\n",
    "        return 'investor_presentation'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Extract documents if page content is available\n",
    "if page_content:\n",
    "    documents = extract_document_links(page_content)\n",
    "    print(f\"Found {len(documents)} potential documents\")\n",
    "    \n",
    "    # Display sample documents\n",
    "    if documents:\n",
    "        df = pd.DataFrame(documents)\n",
    "        print(\"\\nSample documents:\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"⚠️ No documents found - selectors may need adjustment\")\n",
    "else:\n",
    "    print(\"⚠️ Cannot extract documents - no page content available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_document(doc_info, download_dir):\n",
    "    \"\"\"\n",
    "    Download a single document\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(doc_info['url'], headers=HEADERS, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Generate filename\n",
    "        parsed_url = urlparse(doc_info['url'])\n",
    "        filename = f\"{doc_info['type']}_{int(time.time())}_{os.path.basename(parsed_url.path)}\"\n",
    "        \n",
    "        if not filename.endswith('.pdf'):\n",
    "            filename += '.pdf'\n",
    "        \n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        logger.info(f\"Downloaded: {filename}\")\n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'filepath': filepath,\n",
    "            'size_bytes': len(response.content)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download {doc_info['url']}: {e}\")\n",
    "        return {\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Test document download (limit to first 2 documents for testing)\n",
    "if 'documents' in locals() and documents:\n",
    "    test_documents = documents[:2]  # Test with first 2 documents\n",
    "    download_results = []\n",
    "    \n",
    "    for doc in test_documents:\n",
    "        print(f\"Attempting to download: {doc['title']}\")\n",
    "        result = download_document(doc, DOWNLOAD_DIR)\n",
    "        download_results.append(result)\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)  # Respectful scraping\n",
    "    \n",
    "    # Display results\n",
    "    success_count = sum(1 for r in download_results if r['status'] == 'success')\n",
    "    print(f\"\\n✅ Successfully downloaded {success_count}/{len(test_documents)} documents\")\n",
    "    \n",
    "    for i, result in enumerate(download_results):\n",
    "        if result['status'] == 'success':\n",
    "            print(f\"  📄 {test_documents[i]['title']} -> {result['filepath']} ({result['size_bytes']} bytes)\")\n",
    "        else:\n",
    "            print(f\"  ❌ {test_documents[i]['title']} -> {result['error']}\")\n",
    "else:\n",
    "    print(\"⚠️ No documents available for download testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document validation and metadata extraction\n",
    "def validate_downloaded_files():\n",
    "    \"\"\"\n",
    "    Validate downloaded files and extract metadata\n",
    "    \"\"\"\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        print(\"Download directory not found\")\n",
    "        return []\n",
    "    \n",
    "    files = os.listdir(DOWNLOAD_DIR)\n",
    "    pdf_files = [f for f in files if f.endswith('.pdf')]\n",
    "    \n",
    "    file_info = []\n",
    "    for file in pdf_files:\n",
    "        filepath = os.path.join(DOWNLOAD_DIR, file)\n",
    "        try:\n",
    "            stat = os.stat(filepath)\n",
    "            file_info.append({\n",
    "                'filename': file,\n",
    "                'size_bytes': stat.st_size,\n",
    "                'size_mb': round(stat.st_size / (1024*1024), 2),\n",
    "                'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),\n",
    "                'is_valid_pdf': stat.st_size > 1000  # Basic size check\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# Validate downloaded files\n",
    "file_metadata = validate_downloaded_files()\n",
    "if file_metadata:\n",
    "    print(f\"\\n📊 Downloaded file summary:\")\n",
    "    df_files = pd.DataFrame(file_metadata)\n",
    "    print(df_files.to_string(index=False))\n",
    "    \n",
    "    total_size_mb = sum(f['size_mb'] for f in file_metadata)\n",
    "    valid_files = sum(1 for f in file_metadata if f['is_valid_pdf'])\n",
    "    print(f\"\\n📈 Total size: {total_size_mb} MB\")\n",
    "    print(f\"📈 Valid PDFs: {valid_files}/{len(file_metadata)}\")\n",
    "else:\n",
    "    print(\"No files found for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results & Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "1. **Web Scraping**: Test results for screener.in document discovery\n",
    "2. **Document Classification**: Accuracy of automatic document type detection\n",
    "3. **Download Success Rate**: Percentage of successful document downloads\n",
    "4. **File Validation**: Quality and completeness of downloaded files\n",
    "\n",
    "### Improvements Needed:\n",
    "- [ ] Refine CSS selectors for better document discovery\n",
    "- [ ] Add robust error handling for failed downloads\n",
    "- [ ] Implement document content validation (PDF structure check)\n",
    "- [ ] Add duplicate detection and handling\n",
    "- [ ] Create document indexing and search capabilities\n",
    "\n",
    "### Integration Points:\n",
    "- **Table Extraction**: Feed downloaded PDFs to 02_table_extraction.ipynb\n",
    "- **Financial Analysis**: Use classified documents in 03_financial_analysis.ipynb\n",
    "- **RAG Implementation**: Index documents for 05_rag_implementation.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
